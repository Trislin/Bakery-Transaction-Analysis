---
title: "Final Project - Bakery Transaction"
author: "Shuoqi Zhang, Yijing(Trista) Lin"
date: "12/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Include the libraries we are going to need here
library(caret)
library(ggplot2)
library(plyr)
#library(tidyr)
library(tidyverse)
library(ggcorrplot)
library(knitr)
library(splines)
library(glmnet)
library(gridExtra)
library(grid)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(arules) # for Association Rules analysis
library(data.table)
library(gridExtra)
library(tree)
library(factoextra)
library(cluster)
library(arulesViz)
library(DMwR)

nb.cols <- 18
mycolors <- colorRampPalette(brewer.pal(8, "BrBG"))(nb.cols)
cookiecol<-c('#ad6a1d','#9a5327','#cc8d4a','#4e1703','#ecc78d','#2e0a05','#d0dfe4','#33575b', '#173742')

```

# 1 Background

The main dataset is the transaction record of a bakery (https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery). Though not specified in the description of the dataset, it is implied that this bakery is located in the old town of Edinburgh, UK. The dataset is downloaded from kaggle and stored under the same path as this R markdown file.

In order to supplement the dataset with more features, we extract the historical weather records from Meteostat (https://dev.meteostat.net/python/hourly.html#response-parameters). We use the weather data recorded by a weather station in Edinburgh Airport, which is about 8 miles away from the Edinburgh old town. The dataset is downloaded and stored under the same path of this R markdown file.

# 2 Questions To Be Answered

Combining the bakery transaction record and the historical weather record, there are few questions we could further explore, such as:

1. Knowing the date, hour, and weather forecast, to predict the number of transactions during that specific hour.

2. Which items tend to be purchased together?

3. Is there any cluster of the transactions?


# 3 Data Cleaning and Wrangling

## 3.1 Read The Dataset

Firstly we will read the dataset of the bakery transaction record, and the hourly weather record in Edinburgh.

```{r}
df_ori <- read.csv(file = 'BreadBasket_DMS.csv',
               header = TRUE,
               encoding = 'utf-8')

df_weather_ori <- read.csv(file = 'Edinburgh_weather_hourly.csv',
                       header = TRUE,
                       encoding = 'utf-8')

```

Then we display the first few rows of each data set.

```{r}
head(df_ori)
head(df_weather_ori)
```
We will create the keys in order to merge two datasets.

```{r}
df <- df_ori
df_weather <- df_weather_ori
df$Date <- as.Date(df$Date, "%Y-%m-%d")
df$Hour <- as.numeric(substr(df$Time, 1, 2))
df$key <- paste(as.character(df$Date), "@", as.character(df$Hour))
df_weather <- df_weather %>% separate(time, c("Date", "Hour_Minute"), " ")
df_weather <- df_weather %>% separate(Hour_Minute, c("Hour", "Minute"), ":")
df_weather$Date <- as.Date(df_weather$Date, "%m/%d/%Y")
df_weather$Hour <- as.numeric(df_weather$Hour)
df_weather$key <- paste(as.character(df_weather$Date), "@", as.character(df_weather$Hour))

```

## 3.2 The Bakery Transaction Record

### 3.2.1 NA Values and Duplicated Rows

Now we will deal with the data type and missing values, if any. We will start with the bakery transaction record database.

```{r}
summary(df)
```

There is no NAs. Then we will verify whether there are duplicated rows in the dataset

```{r}
dim(df[duplicated(df), ])[1]
```

There are duplicated rows. Considering that our analysis will not focus on the quantities of item sold, it is OK to remove those duplicated rows.

```{r}
df <- distinct(df)
```

### 3.2.2 Weekday vs. Weekend

Given the nature of the bakery business, we may expect different behaviors during weekdays and weekends.

```{r}
df$Weekday <- weekdays(df$Date, abbreviate = TRUE)

df$Weekday <- factor(df$Weekday, 
                        levels = c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'))


res <- ddply(df, ~Weekday, summarise, No_of_transaction = length(unique(Transaction)))

ggplot(data = res, mapping = aes(x = Weekday, y = No_of_transaction, fill = Weekday)) +
  geom_bar(stat = 'identity',width=0.7,alpha=0.8) +
  labs(title = 'No. of Transactions by Weekdays', x = 'Weekdays', y = 'Number of Transactions') +
  scale_fill_brewer(palette = "BrBG") +
  theme_minimal()



```

As expected, there are more transactions on Saturday. However, the number of transactions on Sunday seem to be low. To further understand what happened, we will look at the number of transactions by hour by weekdays.


```{r}
res <- ddply(df, .(Weekday, Hour), summarise, No_of_transaction = length(unique(Transaction)))

ggplot(data = res, mapping = aes(x = as.factor(Hour), y = No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = "identity",alpha=0.8) + facet_wrap(~ Weekday) +
  labs(title = 'No. of Transactions by Hour by Weekdays', x = 'Hours', y = 'Number of Transactions') +
  scale_fill_manual(values = mycolors) +
  theme_minimal() + 
  theme(legend.position = "none",
axis.text = element_text(size=8),
)

```

Looking at the distribution of transactions by hours, we noticed that the trend for Saturday and Sunday are similar and are different from the ones of the other days. Therefore, it makes sense to group Saturday and Sunday together, though Sunday has less transactions comparing with Saturday. This also implies that we may want to split the dataset into weekdays data and weekend data to perform further analysis.

```{r}
df$Weekend <- mapvalues(df$Weekday,
                        from = c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'),
                        to = c(0, 0, 0, 0, 0, 1, 1))


```

### 3.2.3 Hours of The Day

From the figure presented in 3.2.2, we noticed that there are few transactions associated with abnormal hours, such as 1 am and 11 pm.

```{r}
res <- ddply(df, ~Hour, summarise, No_of_transaction = length(unique(Transaction)))
ggplot(data = res, mapping = aes(x = as.factor(Hour), y = No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'No. of Transactions by Hour', x = 'Hours', y = 'Number of Transactions') +
  scale_fill_manual(values = mycolors) +
  theme_minimal() +
  theme(legend.position = "none")


```

Considering the small amount of transactions associated with abnormal hours, we will drop the rows whose hours is outside a normal business operating time. In other words, we will drop rows whose hour is 1, 21, 22, or 23 from our dataset.

```{r}
df <- df%>%filter(Hour<21&Hour>1)
```

Another observation is that the amount of transaction within an hour varies by the time of the day. Here we split the day into two segments: rush hours from 9 to 15 and non-rush hours for the rest of the day. Such split makes sense practically as the period from 9 to 15 covers breakfast, lunch, and coffee or tea time in the afternoon.

```{r}
df$Rush_hours <- mapvalues(df$Hour,
                        from = c(7, 8,
                                 9, 10, 11, 12, 13, 14, 15,
                                 16, 17, 18, 19, 20),
                        to = c(0, 0,
                               1, 1, 1, 1, 1, 1, 1,
                               0, 0, 0, 0, 0))
df$Rush_hours<-as.factor(df$Rush_hours)
```

### 3.2.4 Holidays
We also want to take a look at sales during holidays. We will focus on two holidays, Christmas and New Year. 
0 = non-holiday
1 = Christmas|New Year ï¼ˆ12/24/2016 - 1/1/2017)
Bakery did not open on 12/25/2016, 12/26/2016, 1/1/2017

```{r}
df$Holiday<-0
df$Holiday[df$Date>'2016-12-23'&df$Date<'2017-01-02']<-1
hol<-df%>%group_by(Holiday)%>%summarize(mean_transaction = length(unique(Transaction))/length(unique(Date)))
hol%>%ggplot(aes(as.factor(Holiday),mean_transaction, fill = as.factor(Holiday),label = round(mean_transaction,2))) +
  labs(title = 'Average No. of Transactions by Holiday', x = 'Holiday', y = 'Average Number of Transactions') +
  geom_bar(stat = 'identity', width = 0.5,alpha=0.8) +
  geom_text(vjust = -0.5) +
  scale_fill_manual(values = cookiecol[c(5,7)]) +
  theme_minimal()
df$Holiday<-as.factor(df$Holiday)

```

Indeed, the average number of transaction on holidays is 12% less than the one of non-holidays. 


### 3.2.5 List of Purchased Items

Then we will focus on the "Item" column to understand what are included.

```{r}
Item_tb <- table(df$Item)
sort(Item_tb, decreasing = TRUE)
```

This list of itme seems to be inconsistent and confusing. For example, there are items named "NONE". Also, Brownie is separated from Cakes, Baguette is not considered as Bread, and Medialuna is treated differently from Pastry.

The item type "Adjustment" and "None" are probably introduced by the transaction tracking system or the cashier. In other words, there is no real purchase behind each of them. So we will drop them from the dataset. 
Then, the 'Item_Type' column is reorganized and coded as following: 

Bread = 1

Cookies = 2

Cake|Pastry|Sweets = 3

Coffee = 4

Tea = 5

Hot chocolate|Smoothie|Juice = 6

Other beverage = 7

Meal = 8

Other = 9 

*Ambiguous items are coded as 'Other'

```{r}
df <- df %>% filter(Item!='NONE' & Item!='Adjustment')
df$Item_Type <- 9
df$Item_Type[df$Item%in%c('Bread', 'Farm House', 'Toast','Baguette','Focaccia')]<-1
df$Item_Type[df$Item == 'Cookies']<-2
df$Item_Type[df$Item%in%c('Cake','Pastry','Medialuna','Brownie','Muffin','Alfajores','Scone','Scandinavian','Truffles','Tiffin','Fudge','Jammie Dodgers','Bakewell','Tartine','Vegan mincepie')]<-3
df$Item_Type[df$Item == 'Coffee']<-4
df$Item_Type[df$Item == 'Tea']<-5
df$Item_Type[df$Item%in%c('Hot chocolate', 'Juice', 'Smoothies')]<-6
df$Item_Type[df$Item%in%c('Mineral water', 'Coke')]<-7
df$Item_Type[df$Item%in%c('Sandwich', 'Soup', 'Spanish Brunch', 'Chicken Stew', 'Salad','Frittata')]<-8
df$Item_Type<-as.factor(df$Item_Type)
df%>%group_by(Item_Type)%>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Item_Type,y=Count,fill=Item_Type,label = Count)) +
  geom_bar(stat="identity", width=0.7,alpha=0.7) +
  theme_minimal() + 
  scale_fill_manual(values = cookiecol,labels = c("1:Bread", "2:Cookies", "3:Cake|Pastry|Sweets",'4:Coffee','5:Tea','6:Hot chocolate|Smoothie|Juice','7:Other beverage','8:Meal','9:Other')) +
  geom_text(vjust = -0.5,size=3) + 
  labs(title = 'Item Frequency in Unique Transactions', x = 'Item Type', y = 'Number of Transactions') 
```

From the Figure above, cakes/pastries/sweets are the most popular items in the bakery, followed by Coffee and Bread.

We also want to see if the transaction of each item varies by the hours in the day. We decided to focus on bread, cookies, cake/pastry/sweets, coffee and tea. 

```{r}

bread<-df%>%filter(Item_Type==1)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  facet_wrap(~ Item_Type) +
  labs(title = 'Average Number of Transactions of Bread by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)


cookie<-df%>%filter(Item_Type==2)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cookies by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

pastry<-df%>%filter(Item_Type==3)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cakes/Pastries/Sweets by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

coffee<-df%>%filter(Item_Type==4)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Coffee by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

tea<-df%>%filter(Item_Type==5)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Tea by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

grid.arrange(bread,cookie,pastry,coffee,tea, nrow=3,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours: Bread,Cookies,Pastries,Coffee, and Tea",gp=gpar(fontsize=14,font=3)))



```

The Figure above shows that bread and coffee on average tend to be sold more in the morning (~11am) while tea tends to be sold more in the afternoon. Transaction frequencies of cookies and cakes/pastries/sweet have peaks in both morning and afternoon. 


## 3.3 The Edinburgh Weather Record

Let's start with the high level summary of the dataset.

```{r}
summary(df_weather)
```

There are NA values in some of the columns. The columns "prcp", "snow", "wpgt" and "tsun" contain only NA values, so we will drop them.

The website of Meteostat gives clear explanation of each column (https://dev.meteostat.net/python/hourly.html#response-parameters):

1. station: The Meteo ID of the weather station

2. temp: The air temperature in C

3. dwpt: The dew point in C

4. rhum: The relative humidy in percent

5. wdir: The average wind direction in degrees

6. wspd: The average wind speed in km/h

7. pres: The average sea-level air pressure in hPa

8. coco: The weather condition code (https://dev.meteostat.net/docs/formats.html#weather-condition-codes). Only significant weather events are reported here.

Based on the description of each columns, we could drop the "station" column, as it remains the same for all the observations. We could also drop the "dwpt" column, as we already include the "temp" in our features. Another variable to drop is "pres", as the air pressure is difficult to interpret for people without the background in Meteorology. For the missing values in "coco", we will fill 0, as it stands for non-significant weather events.

We will also remove the "Minute" column as it is always 00.

```{r}
df_weather <- subset(df_weather,
                     select = -c(Minute, prcp, snow, wpgt, tsun, station, dwpt, pres))
```

For the missing values in "wdir" and "wspd", considering the consistency of weather conditions, we will use the value of the hour right after.

```{r}
df_weather$coco[is.na(df_weather$coco)] <- 0

df_weather <- df_weather %>% tidyr::fill(wdir, .direction = "up")
df_weather <- df_weather %>% tidyr::fill(wspd, .direction = "up")


```


## 3.4 The Combined Dataset

We now will merge the two datasets based on the pre-defined keys.

```{r}
df_merged <- merge(x = df, y = df_weather, by = 'key', all.x = TRUE)
df_merged$Date <- df_merged$Date.x
df_merged$Hour <- df_merged$Hour.x
df_merged <- subset(df_merged,
                    select = -c(key, Time, Date.y, Hour.y, Date.x, Hour.x))
df_merged$coco<-as.factor(df_merged$coco)
```


```{r}

df_merged_feature <- subset(df_merged, select = -c(Transaction, Item, Weekday,Weekend, Item_Type, Date,Holiday,coco,Rush_hours))
#Calculate correlation here
corr <- round(cor(df_merged_feature), digits = 2)

#Use ggcorrplot to graph correlation. Only plot the lower triangle of the correlation matrix.
ggcorrplot(corr, type = "lower", 
           ggtheme = ggplot2::theme_minimal,
           lab = TRUE,
           colors = c(cookiecol[5],'white',cookiecol[7]))
```

The correlation table suggests possible correlation between the temperature and the humidity, and between the temperature and the speed of wind.

## 3.5 Dataset Construction

### 3.5.1 Predict Transactions By Day of week, Hour, and Weather Information

```{r}
# To build the dataset for the regression problem
# Keep Hour as a feature
df_merged_reg<-df_merged%>%
  group_by(Date, Hour)%>%
  mutate(No_of_transaction = length(unique(Transaction)))%>%
  ungroup()%>%
  #select(-c(Transaction, Item, Item_Type,Date,Hour))%>%
  select(-c(Transaction, Item, Item_Type,Date))%>%
  unique()%>%
  data.frame()

# Keep Weekday as a feature
df_weekday<-df_merged_reg%>%
  filter(Weekend==0)%>%
  #select(-Weekday,-Weekend)%>%
  select(-Weekend)%>%
  data.frame()

summary(df_weekday)

# Keep Weekday as a feature
df_weekend<-df_merged_reg%>%
  filter(Weekend==1)%>%
  #select(-Weekday,-Weekend)%>%
  select(-Weekend)%>%
  data.frame()

summary(df_weekend)
```

Next, we plot scatter plots of number of transaction against different weather features, respectively.

```{r}
# scatter plots of number of transaction against different weather features, respectively
temp<-df_merged_reg%>%ggplot(aes(temp,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Temperature in Celcius', y='Number of Transaction') + 
  theme_minimal()

ws<-df_merged_reg%>%ggplot(aes(wspd,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Wind Speed', y='Number of Transaction') + 
  theme_minimal()

hum<-df_merged_reg%>%ggplot(aes(rhum,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Humidity', y='Number of Transaction') + 
  theme_minimal()

wcc<-df_merged_reg%>%ggplot(aes(coco,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Weather Condition Code', y='Number of Transaction') + 
  theme_minimal()

grid.arrange(temp,ws,hum,wcc, nrow = 2)



```


### 3.5.2 Market Basket Analysis

Here we will construct the dataset to perform market basket analysis at a later stage.

```{r}
df_merged_mba <- df_merged

# map back the name of Item type

df_merged_mba$Item_Type_Name[df$Item_Type == 1] <- 'Bread'
df_merged_mba$Item_Type_Name[df$Item_Type == 2] <- 'Cookies'
df_merged_mba$Item_Type_Name[df$Item_Type == 3] <- 'Cake|Pastry|Sweets'
df_merged_mba$Item_Type_Name[df$Item_Type == 4] <- 'Coffee'
df_merged_mba$Item_Type_Name[df$Item_Type == 5] <- 'Tea'#'Other beverage'#'Tea'
df_merged_mba$Item_Type_Name[df$Item_Type == 6] <- 'Hot chocolate|Smoothie|Juice'#'Other beverage'
df_merged_mba$Item_Type_Name[df$Item_Type == 7] <- 'Other beverage'
df_merged_mba$Item_Type_Name[df$Item_Type == 8] <- 'Meal'
df_merged_mba$Item_Type_Name[df$Item_Type == 9] <- 'Other'


df_merged_mba_item_type_temp <- subset(df_merged_mba,
                                       select = c(Transaction, Item_Type_Name, Weekend))

df_merged_mba_item_type_temp <- distinct(df_merged_mba_item_type_temp)

# Remove the transactions including only one item type
df_merged_mba_item_type_temp <-
  df_merged_mba_item_type_temp %>%
  group_by(Transaction) %>%
  mutate(freq = n()) %>%
  data.frame()

df_merged_mba_item_type_temp <- df_merged_mba_item_type_temp[df_merged_mba_item_type_temp$freq > 1, ]


# MBA on category of items
df_merged_mba_item_type <- df_merged_mba_item_type_temp %>%
  group_by(Transaction) %>%
  summarise(basket = as.vector(list(Item_Type_Name)))

# MBA on items
df_merged_mba_item <- df_merged_mba %>%
  group_by(Transaction) %>%
  mutate(basket = as.vector(list(Item)))%>%ungroup()%>%data.frame()

mba_wkday<-df_merged_mba_item%>%
  filter(Weekend==0)%>%
  select(-Weekend)

mba_wkend<-df_merged_mba_item%>%
  filter(Weekend==1)%>%
  select(-Weekend)


```

### 3.5.3 Study The Clusters of Transactions

Here we will add the number of type of items for each transaction.

```{r}

df_merged_cluster<-df_merged%>%
  #group_by(Transaction)%>%
  #mutate(No_Item_Type = length(unique(Item)))%>%
  #ungroup()%>%
  select(-Transaction,-Item, -Weekday,-Date)%>%
  unique()%>%
  data.frame()

df_merged_cluster[,c(2:10)]<-sapply(df_merged_cluster[,c(2:10)], as.numeric)

cluster_wkday<-df_merged_cluster%>%filter(Weekend==0)%>%select(-Weekend)%>%data.frame()
cluster_wkend<-df_merged_cluster%>%filter(Weekend==1)%>%select(-Weekend)%>%data.frame()


summary(cluster_wkday)
summary(cluster_wkend)
```



# 4 Modeling and Analysis

## 4.1 Predicting The Number of Transactions

The task of this analysis is to predict the number of transactions during that specific hour, knowing the date, hour, and weather forecast. More importantly, from the model created, we expert to further understand how the sales is driven by external variables, such as weather, day of the week, time of the day and so on.

Firstly, we will split the dataset into training set and validation set.

```{r}
set.seed(0)

train_wkday<-sample(1:nrow(df_weekday), round(0.8*dim(df_weekday)[1]))
train_wkend <- sample(1:nrow(df_weekend), round(0.8*dim(df_weekend)[1]))
num_features <- dim(df_weekday)[2]

```

### 4.1.1 Lasso Regression Model

We will start with Lasso regression model.
**Weekday**
```{r}
x_wkday <- model.matrix(No_of_transaction~.,df_weekday)[,-1]
y_wkday <- df_weekday$No_of_transaction

y.test_wkday <- y_wkday[-train_wkday]

grid=10^seq(10,-2, length =100)
```

```{r}
lasso.mod=glmnet(x_wkday[train_wkday,],y_wkday[train_wkday],alpha=1,lambda=grid)
plot(lasso.mod)
```

```{r}
cv.out=cv.glmnet(x_wkday[train_wkday,],y_wkday[train_wkday],alpha=1)
plot(cv.out)

```

Then we run the lasso model on the test set to see the RMSE.

```{r}
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_wkday[-train_wkday,])
#RMSE
lasso_RMSE_wkday <- sqrt(mean((lasso.pred-y.test_wkday)^2))
lasso_RMSE_wkday
```

```{r}
out <- glmnet(x_wkday,y_wkday,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:8,]
lasso.coef
```

```{r}
lasso.coef[lasso.coef!=0]
```

**Weekend**
```{r}
x_wkend <- model.matrix(No_of_transaction~.,df_weekend)[,-1]
y_wkend <- df_weekend$No_of_transaction

y.test_wkend <- y_wkend[-train_wkend]

grid=10^seq(10,-2, length =100)
```

```{r}
lasso.mod=glmnet(x_wkend[train_wkend,],y_wkend[train_wkend],alpha=1,lambda=grid)
plot(lasso.mod)

```

```{r}
cv.out=cv.glmnet(x_wkend[train_wkend,],y_wkend[train_wkend],alpha=1)
plot(cv.out)
```

The test set RMSE of the model chosen by lasso is

```{r}
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_wkend[-train_wkend,])
#RMSE
lasso_RMSE_wkend <- sqrt(mean((lasso.pred-y.test_wkend)^2))
lasso_RMSE_wkend

```

```{r}
out <- glmnet(x_wkend,y_wkend,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:8,]
lasso.coef
```

```{r}
lasso.coef[lasso.coef!=0]
```

### 4.1.2 Random Forest Modle and GBM Modle

***Random Forest***
Secondly, we try another model - random forest regression model. We will try several different sets of hyperparameters, including 'mtry' and 'ntree' to figure out the best random forest regression model as baseline.
**Weekday**
```{r}
search_grid <- expand.grid(mtry = c(round(num_features/3/2),
                                    round(num_features/3),
                                    round(num_features/3*2)),
                           ntree = c(100, 500, 1000))

min_RMSE <- Inf
best_mtry <- 0
best_ntree <- 0

for (i in seq(1, nrow(search_grid))){
  rf_reg_wkday <- randomForest(No_of_transaction~.,
                           df_weekday[train_wkday, ],
                           mtry = search_grid[i, ]$mtry,
                           ntree = search_grid[i, ]$ntree,
                           importance = TRUE
                           )
  if (mean(rf_reg_wkday$mse) < min_RMSE) {
    min_RMSE <- mean(rf_reg_wkday$mse)
    best_mtry <- search_grid[i, ]$mtry
    best_ntree <- search_grid[i, ]$ntree
    
  }

  }

# Train the random forest model using the best 'mtry' and 'ntree'.
rf_reg_best_wkday <- randomForest(No_of_transaction~.,
                           df_weekday[train_wkday, ],
                           mtry = best_mtry,
                           ntree = best_ntree,
                           importance = TRUE
                           )

rf_reg_best_wkday

```

Then we train a regression trees model using boosting algorithm to compare with the random forest model constructed.
**Weekday**
```{r}
search_grid <- expand.grid(shrinkage = c(0.01, 0.001),
                           interaction.depth = c(4, 5),
                           n.minobsinnode = c(10, 100),
                           bag.fraction = c(0.5, 0.8),
                           optimal_trees = 0,
                           min_RMSE = 0)

for (i in seq(1, nrow(search_grid))){

  boost.fit_wkday <- gbm(No_of_transaction~. ,
                  data = df_weekday[train_wkday, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = search_grid[i, ]$shrinkage,
                  interaction.depth = search_grid[i, ]$interaction.depth,
                  n.minobsinnode = search_grid[i, ]$n.minobsinnode,
                  bag.fraction = search_grid[i, ]$bag.fraction,
                  )
  search_grid[i, ]$min_RMSE <- min(boost.fit_wkday$cv.error)
  search_grid[i, ]$optimal_trees = match(min(boost.fit_wkday$cv.error), boost.fit_wkday$cv.error)

}

# Train and fit this model with the best sets of hyperparameters:

boost_best_para_wkday <- 
  search_grid[search_grid$min_RMSE == min(search_grid$min_RMSE),]

boost.best_wkday <- gbm(No_of_transaction~. ,
                  data = df_weekday[train_wkday, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = boost_best_para_wkday$shrinkage,
                  interaction.depth = boost_best_para_wkday$interaction.depth,
                  n.minobsinnode = boost_best_para_wkday$n.minobsinnode,
                  bag.fraction = boost_best_para_wkday$bag.fraction,
                  )

boost.best_wkday

```

Then we compare the performance of both the random forest model and the trees model using boosting algorithm on the Weekday validation set.

```{r}
boost_pred_test_wkday <- predict(boost.best_wkday,
                           newdata = df_weekday[-train_wkday, ],
                           n.trees = 4000)
boost_RMSE_test_wkday <- caret::RMSE(boost_pred_test_wkday, df_weekday[-train_wkday, ]$No_of_transaction)

rf_pred_test_wkday <- predict(rf_reg_best_wkday,
                        newdata = df_weekday[-train_wkday, ])

rf_RMSE_test_wkday <- caret::RMSE(rf_pred_test_wkday, df_weekday[-train_wkday, ]$No_of_transaction)

print(paste0('RMSE of the boosting model on Weekday testing set:', round(boost_RMSE_test_wkday, 4)))
print(paste0('RMSE of the random forest model on Weekday testing set:', round(rf_RMSE_test_wkday, 4)))
print(paste0('RMSE of the lasso regression model on Weekday testing set:', round(lasso_RMSE_wkday, 4)))

```

For weekdays data, it turns out that the random forest model performs the best on the test set in term of RMSE. So we will choose the random forest model to explain how different features impact the number of transactions in a certain hour during a weekday. For the random forest model, the RMSE on the training set is `r round(sqrt(mean(rf_reg_best_wkday$mse)), 4)`, which doesn't suggest the risk of overfitting. However, one thing worth-noting is that our random forest could only explain 34% of variance. This is probably due to the very limited amount of features we have in our original dataset. 


**Weekend**
```{r}
search_grid <- expand.grid(mtry = c(round(num_features/3/2),
                                    round(num_features/3),
                                    round(num_features/3*2)),
                           ntree = c(100, 500, 1000))

min_RMSE <- Inf
best_mtry <- 0
best_ntree <- 0

for (i in seq(1, nrow(search_grid))){
  rf_reg_wkend <- randomForest(No_of_transaction~.,
                           df_weekend[train_wkend, ],
                           mtry = search_grid[i, ]$mtry,
                           ntree = search_grid[i, ]$ntree,
                           importance = TRUE
                           )
  if (mean(rf_reg_wkend$mse) < min_RMSE) {
    min_RMSE <- mean(rf_reg_wkend$mse)
    best_mtry <- search_grid[i, ]$mtry
    best_ntree <- search_grid[i, ]$ntree
    
  }

  }

# Train the random forest model using the best 'mtry' and 'ntree'.
rf_reg_best_wkend <- randomForest(No_of_transaction~.,
                           df_weekend[train_wkend, ],
                           mtry = best_mtry,
                           ntree = best_ntree,
                           importance = TRUE
                           )

rf_reg_best_wkend

```


```{r}
search_grid <- expand.grid(shrinkage = c(0.01, 0.001),
                           interaction.depth = c(4, 5),
                           n.minobsinnode = c(10, 20),
                           bag.fraction = c(0.5, 0.8),
                           optimal_trees = 0,
                           min_RMSE = 0)

for (i in seq(1, nrow(search_grid))){

  boost.fit_wkend <- gbm(No_of_transaction~. ,
                  data = df_weekend[train_wkend, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = search_grid[i, ]$shrinkage,
                  interaction.depth = search_grid[i, ]$interaction.depth,
                  n.minobsinnode = search_grid[i, ]$n.minobsinnode,
                  bag.fraction = search_grid[i, ]$bag.fraction,
                  )
  search_grid[i, ]$min_RMSE <- min(boost.fit_wkend$cv.error)
  search_grid[i, ]$optimal_trees = match(min(boost.fit_wkend$cv.error), boost.fit_wkend$cv.error)

}

# Train and fit this model with the best sets of hyperparameters:

boost_best_para_wkend <- 
  search_grid[search_grid$min_RMSE == min(search_grid$min_RMSE),]

boost.best_wkend <- gbm(No_of_transaction~. ,
                  data = df_weekend[train_wkend, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = boost_best_para_wkend$shrinkage,
                  interaction.depth = boost_best_para_wkend$interaction.depth,
                  n.minobsinnode = boost_best_para_wkend$n.minobsinnode,
                  bag.fraction = boost_best_para_wkend$bag.fraction,
                  )

boost.best_wkend

```


```{r}
boost_pred_test_wkend <- predict(boost.best_wkend,
                           newdata = df_weekend[-train_wkend, ],
                           n.trees = 4000)
boost_RMSE_test_wkend <- caret::RMSE(boost_pred_test_wkend, df_weekend[-train_wkend, ]$No_of_transaction)

rf_pred_test_wkend <- predict(rf_reg_best_wkend,
                        newdata = df_weekend[-train_wkend, ])

rf_RMSE_test_wkend <- caret::RMSE(rf_pred_test_wkend, df_weekend[-train_wkend, ]$No_of_transaction)

print(paste0('RMSE of the boosting model on Weekend testing set:', round(boost_RMSE_test_wkend, 4)))
print(paste0('RMSE of the random forest model on Weekend testing set:', round(rf_RMSE_test_wkend, 4)))
print(paste0('RMSE of the lasso regression model on Weekend testing set:', round(lasso_RMSE_wkend, 4)))

```


Same as the case of weekday data, for weekend data, it seems that the random forest model selected performs better than the other model selected. For the random forest model, the RMSE on the training set is `r round(sqrt(mean(rf_reg_best_wkend$mse)), 4)`, which doesn't suggest the risk of overfitting. However, only 41% of variace could be explained by the selected random forest model. This is probably again due to the fact that we only have very limited features in our original dataset.

### 4.1.3 Impact of Features

Now that we have the regression model constructed, we would like to further understand the impact of different features.

**Weekday**
```{r, fig.width=7,fig.width=7}
varImpPlot(rf_reg_best_wkday)

```

The most important factors are the time when the transaction happened and whether the time is the rush hours - this is not surprising. What's interesting is that the weather related variables follow right after. We use partial dependence plot here to further understand the impact of weather related variables.

**Weekday**
```{r, fig.width=7,fig.width=7}
par(mfrow = c(2, 2))
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'wdir')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'wspd')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'rhum')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'temp')


```

From the partial dependence plot for Weekday data, we could learn that:

1. Temperature seems to have a U-shape effect on number of transactions. When the temperature is approximately below 2 Celsius, number of transactions decreases as temperature increases; when it is approximately above 2 Celsius, number of transactions increases as temperature increases. Potentially because during weekdays, there could be increasing number of transactions during rush hour, especially in the morning, when the temperature tends to be low. 

2. When there is north wind or north-west wind (270-350), the sales will be higher than the case when the wind is blowing from other directions.

3. Average-level humidity seems to have a positive effect on number of transactions; however, when humidity is close to or over 90, which suggests raining weather, there is a sharp decrease, showing a negative effect on number of transactions. 

4. Stronger wind has negative impact on the sales

To sum up, the most important variable in forecasting the number of transactions at a given time is whether it is during the rush hour. Also, different weather related variables have different impact on the number of transaction - so a practical suggestion to the bakery is to watch the weather forecast and be prepared.

**Weekend**
```{r, fig.width=7,fig.width=7}
varImpPlot(rf_reg_best_wkend)

```

Similar as the case of weekdays, the hour when a transaction happened and whether it is during rush-hours are the most important features. Then the weather related variables also contribute in the model.

**Weekend**
```{r, fig.width=7,fig.width=7}
par(mfrow = c(2, 2))
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'wdir')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'wspd')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'rhum')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'temp')


```

From the partial dependence plot for Weekend data, we could learn that:

1. Interestingly, in contrast to the data in weekdays, temperature no longer has a U-shape effect; instead, overall, as temperature increases, number of transactions increases. That's probably because no one needs to go buy breakfast in a cold Saturday or Sunday morning.

2. In weekends, when there is north wind or north-west wind (270-350), there is a huge increase in number of transactions. 

3. In weekends, as humidity increases, number of transactions decreases overall. Indeed, it decreases dramatically once the humidity is higher than 40. The reason why it is different comparing with weekday would be that only few people will hang out if weather condition is not good in a weekend.

4. Just as weekdays, wind speed has a negative effect on number of transactions.

Comparing with the weekday case, the humidity and temperature impact the number of transactions differently. People have to go to work in weekdays regardless of the weather. But during weekend, the weather condition will play a more important role when people decide whether to go to the bakery.

## 4.2 Market Basket Analysis

The objective of this analysis is to identify possible association rules. For example, assuming we identify from the data that people who buy cake tend to buy cookie at the same time, then the bakery could consider to place these two item closer in the shop, or create bundle sales in order to boost the sales.

We have already re-grouped different items, so for this analysis, we will start from looking at the association rules among item types.

```{r}
transactions_Item_Type <- as(df_merged_mba_item_type$basket, "transactions")
#inspect(transactions_Item_Type[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS
rules_Item_Type <- apriori(transactions_Item_Type, 
                 parameter = list(support = 0.05, 
                                  confidence = 0.025,
                                  minlen = 2))
# Remove redundant rules
rules_Item_Type <- rules_Item_Type[!is.redundant(rules_Item_Type)]

# Reorder by Lift and display
rules_Item_Type_dt <- data.table( lhs = labels( lhs(rules_Item_Type) ), 
                        rhs = labels( rhs(rules_Item_Type) ), 
                        quality(rules_Item_Type) )[ order(-lift), ]
kable(head(rules_Item_Type_dt, 10))

```

From the table we observe that the highest lift of all rules is still smaller than 1. That says, no valid association rules identified among the item types.

Then we focus our market basket analysis on the detail list of items, instead of the category. Here we will analyze weekdays and weekends separately, as we are expecting different purchase behaviors.

### 4.2.1 Weekday Dataset Analysis and Discussion

**Weekday**

```{r}
transactions_Item <- as(mba_wkday$basket, "transactions")
#inspect(transactions_Item[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS

rules_Item <- apriori(transactions_Item, 
                 parameter = list(support = 0.005, 
                                  confidence = 0.25,
                                  minlen = 2)
                 )

# Remove redundant rules
rules_Item <- rules_Item[!is.redundant(rules_Item)]

# Reorder by Lift and display
rules_Item_dt_wkday <- data.table( lhs = labels( lhs(rules_Item) ), 
                        rhs = labels( rhs(rules_Item) ), 
                        quality(rules_Item) )[ order(-lift), ]

kable(head(rules_Item_dt_wkday, 20))


```

```{r}
subrules2 <- head(sort(rules_Item, by = "lift"), 20)
ig <- plot( subrules2, method="graph",
            control=list(type="items") )

```

Few observations could be made from the table:

During weekdays

1. People who buy drinks, such as coffee, juice, tea, and mineral water, tends to buy Sandwich

2. People who buy coffee and juice or juice and tea tends to buy cookies. Normally client won't buy two drinks together if it is only for him or herself. We suspect in this case, it is someone buying drinks and cookies to share with friends.

3. People who buy lunch, such as cake and soup, bread and soup, Spanish brunch, chicken stew, and soup alone, tends to buy tea at the same time. 

4. People who buy sweets, such as scone and cake, tends to buy tea at the same time.

To further explore the finding #4, we plot the distribution of transactions including scone or cake by hours.

```{r}
p1<-mba_wkday%>%filter(Item=='Scone')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkday$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Scone by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p2<-mba_wkday%>%filter(Item=='Cake')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkday$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cake by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)


grid.arrange(p1, p2,
             nrow=1,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours",
                            gp=gpar(fontsize=14,font=3)))

```

It turns out that scone and cake are sold more in the afternoon. The association rules between scone, cake and tea probably refers to the snack in the afternoon.

Based on the above, maybe we could recommend the bakery, for weekdays:

1. To create bundles sales for drinks and sandwiches

2. To create bundles sales for multiple drinks and cookies or cakes

3. To create bundles sales for lunch and tea

4. To create bundles sales for afternoon snack and tea 

### 4.2.2 Weekend Dataset Analysis and Discussion

**Weekend**
```{r}
transactions_Item <- as(mba_wkend$basket, "transactions")
#inspect(transactions_Item[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS
rules_Item_wkend <- apriori(transactions_Item, 
                 parameter = list(support = 0.01, 
                                  confidence = 0.25,
                                  minlen = 2)
                 )

# Remove redundant rules
rules_Item_wkend <- rules_Item_wkend[!is.redundant(rules_Item_wkend)]

# Reorder by Lift and display
rules_Item_dt_wkend <- data.table( lhs = labels( lhs(rules_Item_wkend) ), 
                        rhs = labels( rhs(rules_Item_wkend) ), 
                        quality(rules_Item_wkend) )[ order(-lift), ]

kable(head(rules_Item_dt_wkend, 10))


```

```{r}
# Graph for 10 rules - Weekend
subrules2 <- head(sort(rules_Item_wkend, by = "lift"), 10)
ig <- plot( subrules2, method="graph",
            control=list(type="items") )

```

In weekends, the association rules concentrate on few items, including cake, tea, coffee, hot chocolate, and scone. One thing worth-noting is that cake is in the center of the graph. In other words, people tends to buy cake when they are purchasing bunch of other items. The association rules also suggest that people sometime buy items to share with others, as there are transactions including both coffee and tea or coffee and hot chocolate.

Since cake is the center of the associations, we are curious when the sales of cakes peak and how the sales of different drinks associated with cakes vary by the hours of the day. 

```{r}
p3<-mba_wkend%>%filter(Item=='Cake')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cake by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p4<-mba_wkend%>%filter(Item=='Coffee')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Coffee by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p5<-mba_wkend%>%filter(Item=='Tea')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Tea by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p6<-mba_wkend%>%filter(Item=='Hot chocolate')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Hot chocolate by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

grid.arrange(p3,p4, p5,p6,
             nrow=2,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours",
                            gp=gpar(fontsize=14,font=3)))

```


From the figures above, coffee and hot chocolate are sold more in the morning, as opposed to tea and hot chocolate. Sales of cake peak in the afternoon. It is likely that in weekends, people hang out together at the bakery to get the cake and multiple drinks. The bakery could make a special weekend afternoon combo for cake and drinks.


Based on the observations, the recommendations to the bakery would be:

1. During weekends, place the cake in a visible place closer to the cashier

2. Keep the combo of multiple drinks and cake 


## 4.3 Clustering Analysis and Discussion

In this section, we will explore the different clusters of transactions, if there is any.

### 4.3.1 Clustering on Weekday Dataset

**Weekday**
```{r}

cluster_wkday_scale = scale(cluster_wkday)

fviz_nbclust(
  cluster_wkday_scale,
  kmeans,
  k.max = 10,
  method = "wss"
) + ggtitle ("Elbow Method")


fviz_nbclust(
  cluster_wkday_scale,
  kmeans,
  k.max = 10,
  method = "silhouette"
) + ggtitle ("Silhouette Method")

```

Both figures suggest that the best K should be 6. However, after trying several different K, we decide to go with K = 3.

```{r}
set.seed(0)
km_out_2 <- kmeans(cluster_wkday_scale, 4, nstart = 25)

fviz_cluster(km_out_2, cluster_wkday_scale, geom = 'point', ellipse.type = 'norm',
             main = 'Weekday Dataset: K-Means Clustering Results with K=4') + 
    scale_fill_manual(values = cookiecol[c(7,5,1,4)]) +
  scale_color_manual(values = cookiecol[c(7,5,1,4)]) + 
  theme_minimal()

```

Using PCA, we are able to visualize the clusters in a 2-D plane. Then to understand how well we split the transactions into two clusters, here we compare the centers of the clusters.

```{r}
library('DMwR')
unscale(km_out_2$centers, cluster_wkday_scale)
```


In general, one of the clusters only contains the non-rush hour transactions. Then the other two clusters seem to be split based on weather conditions, including humidity, wind direction and wind speed.

```{r}
cluster_wkday_cbind<-cbind(km_out_2$cluster, cluster_wkday)
cluster_wkday1<-cluster_wkday_cbind%>%filter(km_out_2$cluster==1)
cluster_wkday2<-cluster_wkday_cbind%>%filter(km_out_2$cluster==2)
cluster_wkday3<-cluster_wkday_cbind%>%filter(km_out_2$cluster==3)
cluster_wkday4<-cluster_wkday_cbind%>%filter(km_out_2$cluster==4)
```


```{r}
names(cluster_wkday_cbind)[1]<-'cluster_no'
cluster_wkday_cbind$cluster_no<-as.factor(cluster_wkday_cbind$cluster_no)
cluster_wkday_cbind %>%
  ggplot( aes(x=as.factor(Item_Type),fill=cluster_no)) + facet_wrap(~ cluster_no) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', binwidth = 0.5) +
    scale_fill_brewer(palette = 'BrBG') +
    theme_minimal() 

```


From the graph above, cluster 2 has the highest number of transactions while cluster 3 has the lowest which is explained by the holiday data in cluster 3. The number of transactions of cluster 1,2,4 which all contain non-holiday data, are still quite different. Let's look at the details of other features in these two clusters. 


```{r,fig.height=7}
p1<-cluster_wkday1 %>%
  ggplot(aes(x=as.factor(Item_Type))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[1]) +
    theme_minimal() 
p2<-cluster_wkday1 %>%
  ggplot(aes(x=as.factor(Rush_hours))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[1]) +
    theme_minimal() 
p3<-cluster_wkday1 %>%
  ggplot(aes(x=as.factor(Holiday))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[1]) +
    theme_minimal() 
p4<-cluster_wkday1 %>%
 ggplot(aes(x=temp)) +
    geom_density(color=cookiecol[1], fill=cookiecol[1], alpha=0.6, position = 'identity') +
    theme_minimal() 
p5<-cluster_wkday1 %>%
 ggplot(aes(x=wspd)) +
    geom_density(color=cookiecol[1], fill=cookiecol[1], alpha=0.6, position = 'identity') +
    theme_minimal() 
p6<-cluster_wkday1 %>%
 ggplot(aes(x=rhum)) +
    geom_density(color=cookiecol[1], fill=cookiecol[1], alpha=0.6, position = 'identity') +
    theme_minimal() 
grid.arrange(p1,p2,p3,p4,p5,p6, nrow=4,ncol=2)
```





Cluster 1 contains data in non-rush-hours in non-holidays, with a mean temperature of around 7, relatively low wind speed , high humidity, and low number of transactions in different item types. 

```{r,fig.height=7}
p1<-cluster_wkday2 %>%
  ggplot(aes(x=as.factor(Item_Type))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[5]) +
    theme_minimal() 
p2<-cluster_wkday2 %>%
  ggplot(aes(x=as.factor(Rush_hours))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[5]) +
    theme_minimal() 
p3<-cluster_wkday2 %>%
  ggplot(aes(x=as.factor(Holiday))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[5]) +
    theme_minimal() 
p4<-cluster_wkday2 %>%
 ggplot(aes(x=temp)) +
    geom_density(color=cookiecol[5], fill=cookiecol[5], alpha=0.6, position = 'identity') +
    theme_minimal() 
p5<-cluster_wkday2 %>%
 ggplot(aes(x=wspd)) +
    geom_density(color=cookiecol[5], fill=cookiecol[5], alpha=0.6, position = 'identity') +
    theme_minimal() 
p6<-cluster_wkday2 %>%
 ggplot(aes(x=rhum)) +
    geom_density(color=cookiecol[5], fill=cookiecol[5], alpha=0.6, position = 'identity') +
    theme_minimal() 
p7<-cluster_wkday2 %>%
 ggplot(aes(x=Hour)) +
    geom_density(color=cookiecol[5], fill=cookiecol[5], alpha=0.6, position = 'identity') +
    theme_minimal() 
grid.arrange(p1,p2,p3,p4,p5,p6, nrow=3,ncol=2)
```




Cluster 2 contains data in rush hours in non-holidays, with relatively high temperature, relatively high wind speed, relatively low humidity, and relatively high number of transactions of different items. 

```{r,fig.height=7}
p1<-cluster_wkday3 %>%
  ggplot(aes(x=as.factor(Item_Type))) +
    geom_bar(stat = 'count',alpha=0.8, position = 'identity', fill=cookiecol[7]) +
    theme_minimal() 
p2<-cluster_wkday3 %>%
  ggplot(aes(x=as.factor(Rush_hours))) +
    geom_bar(stat = 'count',alpha=0.8, position = 'identity', fill=cookiecol[7]) +
    theme_minimal() 
p3<-cluster_wkday3 %>%
  ggplot(aes(x=as.factor(Holiday))) +
    geom_bar(stat = 'count',alpha=0.8, position = 'identity', fill=cookiecol[7]) +
    theme_minimal() 
p4<-cluster_wkday3 %>%
 ggplot(aes(x=temp)) +
    geom_density(color=cookiecol[7], fill=cookiecol[7], alpha=0.8, position = 'identity') +
    theme_minimal() 
p5<-cluster_wkday3 %>%
 ggplot(aes(x=wspd)) +
    geom_density(color=cookiecol[7], fill=cookiecol[7], alpha=0.8, position = 'identity') +
    theme_minimal() 
p6<-cluster_wkday3 %>%
 ggplot(aes(x=rhum)) +
    geom_density(color=cookiecol[7], fill=cookiecol[7], alpha=0.8, position = 'identity') +
    theme_minimal() 
grid.arrange(p1,p2,p3,p4,p5,p6, nrow=3,ncol=2)
```


Cluster 3 contains holiday data which has even lower number of transactions comparing to cluster 1 (non-rush hour in non-holiday) even though temperature is relatively higher, humidity is relatively lower.


```{r,fig.height=7}
p1<-cluster_wkday4 %>%
  ggplot(aes(x=as.factor(Item_Type))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[8]) +
    theme_minimal() 
p2<-cluster_wkday4 %>%
  ggplot(aes(x=as.factor(Rush_hours))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[8]) +
    theme_minimal() 
p3<-cluster_wkday4 %>%
  ggplot(aes(x=as.factor(Holiday))) +
    geom_bar(stat = 'count',alpha=0.6, position = 'identity', fill=cookiecol[8]) +
    theme_minimal() 
p4<-cluster_wkday4 %>%
 ggplot(aes(x=temp)) +
    geom_density(color=cookiecol[8], fill=cookiecol[8], alpha=0.6, position = 'identity') +
    theme_minimal() 
p5<-cluster_wkday4 %>%
 ggplot(aes(x=wspd)) +
    geom_density(color=cookiecol[8], fill=cookiecol[8], alpha=0.6, position = 'identity') +
    theme_minimal() 
p6<-cluster_wkday4 %>%
 ggplot(aes(x=rhum)) +
    geom_density(color=cookiecol[8], fill=cookiecol[8], alpha=0.6, position = 'identity') +
    theme_minimal() 
grid.arrange(p1,p2,p3,p4,p5,p6, nrow=3,ncol=2)
```


cluster 4 represents (mostly) rush-hours in non-holiday, with a mean temperature of close to 5, relatively low wind speed, relatively high humidity, and relatively higher number of transactions in different item types.

In conclusion: 

Since all clusters except for cluster 1 have quite a lot overlapping, we can only obtain limited information from the clustering results now. We can only say that lower number of transactions tends to happen in holidays as opposed to non-holidays. 

### 4.3.2 Clustering on Weekend Dataset

**Weekend**

```{r}
cluster_wkend_scale = scale(cluster_wkend)

fviz_nbclust(
  cluster_wkend_scale,
  kmeans,
  k.max = 10,
  method = "wss"
) + ggtitle ("Elbow Method")


fviz_nbclust(
  cluster_wkend_scale,
  kmeans,
  k.max = 10,
  method = "silhouette"
) + ggtitle ("Silhouette Method")

```



Both figures suggest that the best K should be 8. However, after trying several differnt K, we select K = 3.

```{r}
set.seed(0)
km_out_2 <- kmeans(cluster_wkend_scale, 3, nstart = 25)

fviz_cluster(km_out_2, cluster_wkend_scale, geom = 'point', ellipse.type = 'norm',
             main = 'Weekend Dataset: K-Means Clustering Results with K=3') + 
  scale_fill_manual(values = cookiecol[c(5,1,7)]) +
  scale_color_manual(values = cookiecol[c(5,1,7)]) + 
  theme_minimal()

```




Then to understand how well we split the transactions into two clusters, here we compare the centers of the clusters.

```{r}
unscale(km_out_2$centers, cluster_wkend_scale)

```

```{r}
cluster_wkend_cbind<-cbind(km_out_2$cluster, cluster_wkend)
cluster_wkend1<-cluster_wkend_cbind%>%filter(km_out_2$cluster==1)
cluster_wkend2<-cluster_wkend_cbind%>%filter(km_out_2$cluster==2)
cluster_wkend3<-cluster_wkend_cbind%>%filter(km_out_2$cluster==3)
```


```{r}
names(cluster_wkend_cbind)[1]<-'cluster_no'
cluster_wkend_cbind$cluster_no<-as.factor(cluster_wkend_cbind$cluster_no)
cluster_wkend_cbind %>%
  ggplot( aes(x=as.factor(Item_Type),fill=cluster_no)) + facet_wrap(~ cluster_no) +
    geom_bar(stat = 'count',alpha=0.8, position = 'identity', binwidth = 0.5) +
    scale_fill_manual(values = cookiecol[c(5,1,7)]) +
    xlab('Item Type') + 
    ylab('Number of Transactions') + 
    theme_minimal() 

```




From the graph above, cluster 2 has the highest number of transactions while cluster 3 has the lowest which is explained by the holiday data in cluster 3. The number of transactions of cluster 1 and cluster 2, which contain both non-holiday data, are still quite different. Let's look at the details of other features in these two clusters. 

```{r}
cluster_wkend_cbind$cluster_no<-as.factor(cluster_wkend_cbind$cluster_no)
p1<-cluster_wkend_cbind %>%
  filter(cluster_no%in%c(1,2)) %>%
  ggplot(aes(x=Hour, fill=cluster_no)) + 
    geom_density( color="#e9ecef", alpha=0.8, position = 'identity') +
    scale_fill_manual(values = cookiecol[c(5,1)]) +
    theme_minimal() +
    labs(fill="") 

p2<-cluster_wkend_cbind %>%
  filter(cluster_no%in%c(1,2)) %>%
  ggplot(aes(x=temp, fill=cluster_no)) + 
    geom_density( color="#e9ecef", alpha=0.8, position = 'identity') +
    scale_fill_manual(values = cookiecol[c(5,1)]) +
    theme_minimal() +
    labs(fill="") +
    xlab('Temperature') 

p3<-cluster_wkend_cbind %>%
  filter(cluster_no%in%c(1,2)) %>%
  ggplot(aes(x=wspd, fill=cluster_no)) + 
    geom_density( color="#e9ecef", alpha=0.8, position = 'identity') +
    scale_fill_manual(values = cookiecol[c(5,1)]) +
    theme_minimal() +
    labs(fill="") +
    xlab('Wind Speed') 

p4<-cluster_wkend_cbind %>%
  filter(cluster_no%in%c(1,2)) %>%
  ggplot(aes(x=rhum, fill=cluster_no)) + 
    geom_density( color="#e9ecef", alpha=0.8, position = 'identity') +
    scale_fill_manual(values = cookiecol[c(5,1)]) +
    theme_minimal() +
    labs(fill="") +
    xlab('Humidity') 

grid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)


```





Cluster 2 tends to contain data in the afternoons, with relatively higher temperature, higher wind speed and relatively lower humidity. 
However, there is still a lot of overlapping between two clusters, so limited information could be derived from the clustering results now. 


# 5 Conclusion & Next Steps

In a nutshell, with the bakery transaction data and weather record data, we first separated weekdays from weekend. Then we explored how different features influencing the number of transactions, especially those weather-related variables. After that we dived into the association rules from the transaction history, identified purchasing patterns, and provided recommendations for the bakery to further increase sales. In the end we tried to cluster the transactions into different segments, and discussed the initial results.
 
The major next steps of this analysis are:
 
1. When predicting the number of transactions, the selected random forest model could only explain 30% - 40% of variance. We will need to either acquire more data beyond Oct 2016 and Apr 2017, or get more new features. Our current features only include date, time, name of items, and weather. If we could manage to get the information of the client, we could further improve our model to perform the prediction task.
 
2. The association rules we identified has low support in general. If we could get more data, we might be able to further explore the the possible rules.
 
3. We could dive deeper into the clustering analysis, to further explore the similarities of transactions within the same cluster. If we could acquire more features, such as the demographic information about the client, we could further explore the clustering analysis and eventually turn the findings into executable actions to the bakery.
 
4. Whether a day is a public holiday or not is not significant in any of our analysis. Part of the reason is that our dataset only lasts 6 months. If we could acquire more data, covering several years, we could further explore the impact of holiday on the sales of the bakery.


