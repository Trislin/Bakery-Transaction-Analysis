---
title: "Final Project - Bakery Transaction"
author: "Shuoqi Zhang, Yijing(Trista) Lin"
date: "11/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Include the libraries we are going to need here
library(caret)
library(ggplot2)
library(plyr)
#library(tidyr)
library(tidyverse)
library(ggcorrplot)
library(knitr)
library(splines)
library(glmnet)
library(gridExtra)
library(grid)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(arules) # for Association Rules analysis
library(data.table)
library(gridExtra)
library(tree)
library(factoextra)
library(cluster)

nb.cols <- 18
mycolors <- colorRampPalette(brewer.pal(8, "Blues"))(nb.cols)
cookiecol<-c('#ad6a1d','#9a5327','#cc8d4a','#4e1703','#ecc78d','#2e0a05','#d0dfe4','#33575b', '#173742')

```

#1 Background

The main dataset is the transaction record of a bakery (https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery). Though not specified in the description of the dataset, it is implied that this bakery is located in the old town of Edinburgh, UK. The dataset is downloaded from kaggle and stored under the same path as this R markdown file.

In order to supplement the dataset with more features, we extract the historical weather records from Meteostat (https://dev.meteostat.net/python/hourly.html#response-parameters). We use the weather data recorded by a weather station in Edinburgh Airport, which is about 8 miles away from the Edinburgh old town. The dataset is downloaded and stored under the same path of this R markdown file.

#2 Questions To Be Answered

Combining the bakery transaction record and the historical weather record, there are few questions we could further explore, such as:

1. Knowing the date, hour, and weather forecast, to predict the number of transactions during that specific hour.

2. Knowing the date, hour, weather forecast and other information, to predict whether a transaction will include the purchase of any kind of bread.

3. Are there any items which are always brought together?


#3 Data Cleaning and Wrangling

## 3.1 Read The Dataset

Firstly we will read the dataset of the bakery transaction record, and the hourly weather record in Edinburgh.

```{r}
df_ori <- read.csv(file = 'BreadBasket_DMS.csv',
               header = TRUE,
               encoding = 'utf-8')

df_weather_ori <- read.csv(file = 'Edinburgh_weather_hourly.csv',
                       header = TRUE,
                       encoding = 'utf-8')

```

Then we display the first few rows of each data set.

```{r}
head(df_ori)
head(df_weather_ori)
```
We will create the keys in order to merge two datasets.

```{r}
df <- df_ori
df_weather <- df_weather_ori
df$Date <- as.Date(df$Date, "%Y-%m-%d")
df$Hour <- as.numeric(substr(df$Time, 1, 2))
df$key <- paste(as.character(df$Date), "@", as.character(df$Hour))
df_weather <- df_weather %>% separate(time, c("Date", "Hour_Minute"), " ")
df_weather <- df_weather %>% separate(Hour_Minute, c("Hour", "Minute"), ":")
df_weather$Date <- as.Date(df_weather$Date, "%m/%d/%Y")
df_weather$Hour <- as.numeric(df_weather$Hour)
df_weather$key <- paste(as.character(df_weather$Date), "@", as.character(df_weather$Hour))

```

## 3.2 The Bakery Transaction Record

### 3.2.1 NA Values and Duplicated Rows

Now we will deal with the data type and missing values, if any. We will start with the bakery transaction record database.

```{r}
summary(df)
```

There is no NAs. Then we will verify whether there are duplicated rows in the dataset

```{r}
dim(df[duplicated(df), ])[1]
```

There are duplicated rows. Considering that our analysis will not focus on the quantities of item sold, it is OK to remove those duplicated rows.

```{r}
df <- distinct(df)
```

### 3.2.2 Weekday vs. Weekend

Given the nature of the bakery business, we may expect different behaviors during weekdays and weekends.

```{r}
df$Weekday <- weekdays(df$Date, abbreviate = TRUE)

df$Weekday <- factor(df$Weekday, 
                        levels = c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'))


res <- ddply(df, ~Weekday, summarise, No_of_transaction = length(unique(Transaction)))

ggplot(data = res, mapping = aes(x = Weekday, y = No_of_transaction, fill = Weekday)) +
  geom_bar(stat = 'identity',width=0.7,alpha=0.8) +
  labs(title = 'No. of Transactions by Weekdays', x = 'Weekdays', y = 'Number of Transactions') +
  scale_fill_brewer(palette = "Blues") +
  theme_minimal()



```

As expected, there are more transactions on Saturday. However, the number of transactions on Sunday seem to be low. To further understand what happened, we will look at the number of transactions by hour by weekdays.


```{r}
res <- ddply(df, .(Weekday, Hour), summarise, No_of_transaction = length(unique(Transaction)))

ggplot(data = res, mapping = aes(x = as.factor(Hour), y = No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = "identity",alpha=0.8) + facet_wrap(~ Weekday) +
  labs(title = 'No. of Transactions by Hour by Weekdays', x = 'Hours', y = 'Number of Transactions') +
  scale_fill_manual(values = mycolors) +
  theme_minimal() + 
  theme(legend.position = "none",
axis.text = element_text(size=8),
)

```

Looking at the distribution of transactions by hours, we noticed that the trend for Saturday and Sunday are similar and are different from the ones of the other days. Therefore, it makes sense to group Saturday and Sunday together, though Sunday has less transactions comparing with Saturday.

```{r}
df$Weekend <- mapvalues(df$Weekday,
                        from = c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'),
                        to = c(0, 0, 0, 0, 0, 1, 1))


```

### 3.2.3 Hours of The Day

From the figure presented in 3.2.2, we noticed that there are few transactions associated with abnormal hours, such as 1 am and 11 pm.

```{r}
res <- ddply(df, ~Hour, summarise, No_of_transaction = length(unique(Transaction)))
ggplot(data = res, mapping = aes(x = as.factor(Hour), y = No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'No. of Transactions by Hour', x = 'Hours', y = 'Number of Transactions') +
  scale_fill_manual(values = mycolors) +
  theme_minimal() +
  theme(legend.position = "none")


```

Considering the small amount of transactions associated with abnormal hours, we will drop the rows whose hours is outside a normal business operating time. In other words, we will drop rows whose hour is 1, 21, 22, or 23 from our dataset.

```{r}
df <- df%>%filter(Hour<21&Hour>1)
```

Another observation is that the amount of transaction within an hour varies by the time of the day. Here we split the day into two segments: rush hours from 9 to 15 and non-rush hours for the rest of the day. Such split makes sense practically as the period from 9 to 15 covers breakfast, lunch, and coffee or tea time in the afternoon.

```{r}
df$Rush_hours <- mapvalues(df$Hour,
                        from = c(7, 8,
                                 9, 10, 11, 12, 13, 14, 15,
                                 16, 17, 18, 19, 20),
                        to = c(0, 0,
                               1, 1, 1, 1, 1, 1, 1,
                               0, 0, 0, 0, 0))
df$Rush_hours<-as.factor(df$Rush_hours)
```

### 3.2.4 Holidays
We also want to take a look at sales during holidays. We will focus on two holidays, Christmas and New Year. 
0 = non-holiday
1 = Christmas|New Year ï¼ˆ12/24/2016 - 1/1/2017)
Bakery did not open on 12/25/2016, 12/26/2016, 1/1/2017

```{r}
df$Holiday<-0
df$Holiday[df$Date>'2016-12-23'&df$Date<'2017-01-02']<-1
hol<-df%>%group_by(Holiday)%>%summarize(mean_transaction = length(unique(Transaction))/length(unique(Date)))
hol%>%ggplot(aes(as.factor(Holiday),mean_transaction, fill = as.factor(Holiday),label = round(mean_transaction,2))) +
  labs(title = 'Average No. of Transactions by Holiday', x = 'Holiday', y = 'Average Number of Transactions') +
  geom_bar(stat = 'identity', width = 0.5,alpha=0.8) +
  geom_text(vjust = -0.5) +
  scale_fill_manual(values = cookiecol[c(5,7)]) +
  theme_minimal()
df$Holiday<-as.factor(df$Holiday)

```

Indeed, the average number of transaction on holidays is 12% less than the one of non-holidays. 


### 3.2.5 List of Purchased Items

Then we will focus on the "Item" column to understand what are included.

```{r}
Item_tb <- table(df$Item)
sort(Item_tb, decreasing = TRUE)
```

This list of itme seems to be inconsistent and confusing. For example, there are items named "NONE". Also, Brownie is separated from Cakes, Baguette is not considered as Bread, and Medialuna is treated differently from Pastry.

The item type "Adjustment" and "None" are probably introduced by the transaction tracking system or the cashier. In other words, there is no real purchase behind each of them. So we will drop them from the dataset. 
Then, the 'Item_Type' column is reorganized and coded as following: 

Bread = 1

Cookies = 2

Cake|Pastry|Sweets = 3

Coffee = 4

Tea = 5

Hot chocolate|Smoothie|Juice = 6

Other beverage = 7

Meal = 8

Other = 9 

*Ambiguous items are coded as 'Other'

```{r}
df <- df %>% filter(Item!='NONE' & Item!='Adjustment')
df$Item_Type <- 9
df$Item_Type[df$Item%in%c('Bread', 'Farm House', 'Toast','Baguette','Focaccia')]<-1
df$Item_Type[df$Item == 'Cookies']<-2
df$Item_Type[df$Item%in%c('Cake','Pastry','Medialuna','Brownie','Muffin','Alfajores','Scone','Scandinavian','Truffles','Tiffin','Fudge','Jammie Dodgers','Bakewell','Tartine','Vegan mincepie')]<-3
df$Item_Type[df$Item == 'Coffee']<-4
df$Item_Type[df$Item == 'Tea']<-5
df$Item_Type[df$Item%in%c('Hot chocolate', 'Juice', 'Smoothies')]<-6
df$Item_Type[df$Item%in%c('Mineral water', 'Coke')]<-7
df$Item_Type[df$Item%in%c('Sandwich', 'Soup', 'Spanish Brunch', 'Chicken Stew', 'Salad','Frittata')]<-8
df$Item_Type<-as.factor(df$Item_Type)
df%>%group_by(Item_Type)%>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Item_Type,y=Count,fill=Item_Type,label = Count)) +
  geom_bar(stat="identity", width=0.7,alpha=0.7) +
  theme_minimal() + 
  scale_fill_manual(values = cookiecol,labels = c("1:Bread", "2:Cookies", "3:Cake|Pastry|Sweets",'4:Coffee','5:Tea','6:Hot chocolate|Smoothie|Juice','7:Other beverage','8:Meal','9:Other')) +
  geom_text(vjust = -0.5,size=3) + 
  labs(title = 'Item Frequency in Unique Transactions', x = 'Item Type', y = 'Average Number of Transactions') 
```

From the Figure above, cakes/pastries/sweets are the most popular items in the bakery, followed by Coffee and Bread.

We also want to see if the transaction of each item varies by the hours in the day. We decided to focus on bread, cookies, cake/pastry/sweets, coffee and tea. 

```{r}

bread<-df%>%filter(Item_Type==1)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  facet_wrap(~ Item_Type) +
  labs(title = 'Average Number of Transactions of Bread by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)


cookie<-df%>%filter(Item_Type==2)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cookies by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

pastry<-df%>%filter(Item_Type==3)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cakes/Pastries/Sweets by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

coffee<-df%>%filter(Item_Type==4)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Coffee by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

tea<-df%>%filter(Item_Type==5)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Tea by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

grid.arrange(bread,cookie,pastry,coffee,tea, nrow=3,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours: Bread,Cookies,Pastries,Coffee, and Tea",gp=gpar(fontsize=14,font=3)))



```

The Figure above shows that bread and coffee on average tend to be sold more in the morning (~11am) while tea tends to be sold more in the afternoon. Transaction frequencies of cookies and cakes/pastries/sweet have peaks in both morning and afternoon. 


## 3.3 The Edinburgh Weather Record

Let's start with the high level summary of the dataset.

```{r}
summary(df_weather)
```

There are NA values in some of the columns. The columns "prcp", "snow", "wpgt" and "tsun" contain only NA values, so we will drop them.

The website of Meteostat gives clear explanation of each column (https://dev.meteostat.net/python/hourly.html#response-parameters):

1. station: The Meteo ID of the weather station

2. temp: The air temperature in C

3. dwpt: The dew point in C

4. rhum: The relative humidy in percent

5. wdir: The average wind direction in degrees

6. wspd: The average wind speed in km/h

7. pres: The average sea-level air pressure in hPa

8. coco: The weather condition code (https://dev.meteostat.net/docs/formats.html#weather-condition-codes). Only significant weather events are reported here.

Based on the description of each columns, we could drop the "station" column, as it remains the same for all the observations. We could also drop the "dwpt" column, as we already include the "temp" in our features. Another variable to drop is "pres", as the air pressure is difficult to interpret for people without the background in Meteorology. For the missing values in "coco", we will fill 0, as it stands for non-significant weather events.

We will also remove the "Minute" column as it is always 00.

```{r}
df_weather <- subset(df_weather,
                     select = -c(Minute, prcp, snow, wpgt, tsun, station, dwpt, pres))
```

For the missing values in "wdir" and "wspd", considering the consistency of weather conditions, we will use the value of the hour right after.

```{r}
df_weather$coco[is.na(df_weather$coco)] <- 0

df_weather <- df_weather %>% tidyr::fill(wdir, .direction = "up")
df_weather <- df_weather %>% tidyr::fill(wspd, .direction = "up")
#df_weather <- df_weather %>% tidyr::fill(pres, .direction = "up")

```


## 3.4 The Combined Dataset

We now will merge the two datasets based on the pre-defined keys.

```{r}
df_merged <- merge(x = df, y = df_weather, by = 'key', all.x = TRUE)
df_merged$Date <- df_merged$Date.x
df_merged$Hour <- df_merged$Hour.x
df_merged <- subset(df_merged,
                    select = -c(key, Time, Date.y, Hour.y, Date.x, Hour.x))
df_merged$coco<-as.factor(df_merged$coco)
```


```{r}

df_merged_feature <- subset(df_merged, select = -c(Transaction, Item, Weekday,Weekend, Item_Type, Date,Holiday,coco,Rush_hours))
#Calculate correlation here
corr <- round(cor(df_merged_feature), digits = 2)

#Use ggcorrplot to graph correlation. Only plot the lower triangle of the correlation matrix.
ggcorrplot(corr, type = "lower", 
           ggtheme = ggplot2::theme_minimal,
           lab = TRUE,
           colors = c(cookiecol[5],'white',cookiecol[7]))
```

The correlation table suggests possible correlation between the temperature and the humidity, and between the temperature and the speed of wind.

## 3.5 Dataset Construction

### 3.5.1 Predict Transactions By Day of week, Hour, and Weather Information

```{r}
# To build the dataset for the regression problem
df_merged_reg<-df_merged%>%group_by(Date, Hour)%>%mutate(No_of_transaction = length(unique(Transaction)))%>%
  ungroup()%>%select(-c(Transaction, Item, Item_Type,Date,Hour))%>%unique()%>%data.frame()




#df_merged_reg <- ddply(df_merged, .(Date, Hour), summarise, No_of_transaction = length(unique(Transaction)))


#df_merged_temp <- subset(df_merged, select = -c(Transaction, Item, Item_Type))

#df_merged_reg <- merge(x = df_merged_reg,
#                       y = df_merged_temp,
#                       by = c('Date', 'Hour'))

#df_merged_reg <- distinct(df_merged_reg)

#df_merged_reg <- subset(df_merged_reg, select = -c(Date))

df_weekday<-df_merged_reg%>%filter(Weekend==0)%>%select(-Weekday,-Weekend)%>%data.frame()
summary(df_weekday)
df_weekend<-df_merged_reg%>%filter(Weekend==1)%>%select(-Weekday,-Weekend)%>%data.frame()
summary(df_weekend)
```

Next, we plot scatter plots of number of transaction against different weather features, respectively
```{r}
# scatter plots of number of transaction against different weather features, respectively
temp<-df_merged_reg%>%ggplot(aes(temp,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Temperature in Celcius', y='Number of Transaction') + 
  theme_minimal()

ws<-df_merged_reg%>%ggplot(aes(wspd,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Wind Speed', y='Number of Transaction') + 
  theme_minimal()

hum<-df_merged_reg%>%ggplot(aes(rhum,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Humidity', y='Number of Transaction') + 
  theme_minimal()

wcc<-df_merged_reg%>%ggplot(aes(coco,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Weather Condition Code', y='Number of Transaction') + 
  theme_minimal()

grid.arrange(temp,ws,hum,wcc, nrow = 2)



```


### 3.5.3 Market Basket Analysis

Here we will construct the dataset to perform market basket analysis at a later stage.

```{r}
df_merged_mba <- df_merged

# map back the name of Item type

df_merged_mba$Item_Type_Name[df$Item_Type == 1] <- 'Bread'
df_merged_mba$Item_Type_Name[df$Item_Type == 2] <- 'Cookies'
df_merged_mba$Item_Type_Name[df$Item_Type == 3] <- 'Cake|Pastry|Sweets'
df_merged_mba$Item_Type_Name[df$Item_Type == 4] <- 'Coffee'
df_merged_mba$Item_Type_Name[df$Item_Type == 5] <- 'Tea'#'Other beverage'#'Tea'
df_merged_mba$Item_Type_Name[df$Item_Type == 6] <- 'Hot chocolate|Smoothie|Juice'#'Other beverage'
df_merged_mba$Item_Type_Name[df$Item_Type == 7] <- 'Other beverage'
df_merged_mba$Item_Type_Name[df$Item_Type == 8] <- 'Meal'
df_merged_mba$Item_Type_Name[df$Item_Type == 9] <- 'Other'


df_merged_mba_item_type_temp <- subset(df_merged_mba,
                                       select = c(Transaction, Item_Type_Name, Weekend))

df_merged_mba_item_type_temp <- distinct(df_merged_mba_item_type_temp)

# Remove the transactions including only one item type
df_merged_mba_item_type_temp <-
  df_merged_mba_item_type_temp %>%
  group_by(Transaction) %>%
  mutate(freq = n()) %>%
  data.frame()

df_merged_mba_item_type_temp <- df_merged_mba_item_type_temp[df_merged_mba_item_type_temp$freq > 1, ]


# MBA on category of items
df_merged_mba_item_type <- df_merged_mba_item_type_temp %>%
  group_by(Transaction) %>%
  summarise(basket = as.vector(list(Item_Type_Name)))

# MBA on items
df_merged_mba_item <- df_merged_mba %>%
  group_by(Transaction) %>%
  mutate(basket = as.vector(list(Item)))%>%ungroup()%>%data.frame()

mba_wkday<-df_merged_mba_item%>%filter(Weekend==0)%>%select(-Weekend)
mba_wkend<-df_merged_mba_item%>%filter(Weekend==1)%>%select(-Weekend)


```

### 3.5.4 Study The Clusters of Transactions

On top of the database constructed during chapter 3.5.2, here we will add the number of type of items for each transaction.

```{r}
#df_merged_cluster <- ddply(df_merged,
#                           ~Transaction,
#                           summarise,
#                           No_Item_Type = length(unique(Item)))

#df_merged_cluster <- merge(x = df_merged_cluster,
                           #y = df_merged_clas,
                           #by = 'Transaction')

#df_merged_cluster <- distinct(df_merged_cluster)

# remove the transaction ID and Weekday
#df_merged_cluster <- subset(df_merged_cluster,
                            #select = -c(Transaction, Weekday))

#summary(df_merged_cluster)

```



#4 Modeling and Analysis

##4.1 Predicting The Number of Transactions

The task of this analysis is to predict the number of transactions during that specific hour, knowing the date, hour, and weather forecast. More importantly, from the model created, we expert to further understand how the sales is driven by external variables, such as weather, day of the week, time of the day and so on.

Firstly, we will split the dataset into training set and validation set.

```{r}
set.seed(0)

# Here we will drop some features from the model

#df_merged_reg <- subset(df_merged_reg,
#                        #select = -c(Weekday))
#                        select = -c(Weekday, Hour))

train_wkday<-sample(1:nrow(df_weekday), round(0.8*dim(df_weekday)[1]))
train_wkend <- sample(1:nrow(df_weekend), round(0.8*dim(df_weekend)[1]))
num_features <- dim(df_weekday)[2]

```

Lasso- I want to explore what model Lasso will provide
**Weekday**
```{r}
x_wkday <- model.matrix(No_of_transaction~.,df_weekday)[,-1]
y_wkday <- df_weekday$No_of_transaction

y.test_wkday <- y_wkday[-train_wkday]

grid=10^seq(10,-2, length =100)
```

```{r}
lasso.mod=glmnet(x_wkday[train_wkday,],y_wkday[train_wkday],alpha=1,lambda=grid)
plot(lasso.mod)

```

```{r}
cv.out=cv.glmnet(x_wkday[train_wkday,],y_wkday[train_wkday],alpha=1)
plot(cv.out)
```

test MSE of the model chosen by lasso
```{r}
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_wkday[-train_wkday,])
mean((lasso.pred-y.test_wkday)^2)

```

```{r}
out <- glmnet(x_wkday,y_wkday,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:8,]
lasso.coef
```

```{r}
lasso.coef[lasso.coef!=0]
```
In weekdays, rush hours has the largest effects on number of transactions. The number of transactions in rush hours is more than the number of transactions not in rush hours by 3.73. The number of transactions in holidays is less than the number of transactions not in holidays by 0.15. One unit increase in temperature increases the number of transactions by 0.001. One unit increase in wind speed decreases the number of transactions by 0.017. When weather condition is foggy(coco=5), number of transactions is lower than number of transactions in other weather conditions. 

**Weekend**
```{r}
x_wkend <- model.matrix(No_of_transaction~.,df_weekend)[,-1]
y_wkend <- df_weekend$No_of_transaction

y.test_wkend <- y_wkend[-train_wkend]

grid=10^seq(10,-2, length =100)
```

```{r}
lasso.mod=glmnet(x_wkend[train_wkend,],y_wkend[train_wkend],alpha=1,lambda=grid)
plot(lasso.mod)

```

```{r}
cv.out=cv.glmnet(x_wkend[train_wkend,],y_wkend[train_wkend],alpha=1)
plot(cv.out)
```

test MSE of the model chosen by lasso
```{r}
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_wkend[-train_wkend,])
mean((lasso.pred-y.test_wkend)^2)

```

```{r}
out <- glmnet(x_wkend,y_wkend,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:8,]
lasso.coef
```

```{r}
lasso.coef[lasso.coef!=0]
```
In weekends, rush hours again has the strongest effect on the number of transactions. Wind speed still has a negative effect on the number of transactions. In contrast to weekdays, temperature now has a negative effect on the number of transactions (consider this data mostly came from data in winter...??). When weather condition is foggy(coco=5), number of transactions is higher than number of transactions in other weather conditions. Finally, wind direction only has a weak positive effect on the number of transactions for transactions in weekends. 

***Random Forest***
Secondly, we set up our baseline regression model - random forest regression model. We will try several different sets of hyperparameters, including 'mtry' and 'ntree' to figure out the best random forest regression model as baseline.
**Weekday**
```{r}
search_grid <- expand.grid(mtry = c(round(num_features/3/2),
                                    round(num_features/3),
                                    round(num_features/3*2)),
                           ntree = c(100, 500, 1000))

min_RMSE <- Inf
best_mtry <- 0
best_ntree <- 0

for (i in seq(1, nrow(search_grid))){
  rf_reg_wkday <- randomForest(No_of_transaction~.,
                           df_weekday[train_wkday, ],
                           mtry = search_grid[i, ]$mtry,
                           ntree = search_grid[i, ]$ntree,
                           importance = TRUE
                           )
  if (mean(rf_reg_wkday$mse) < min_RMSE) {
    min_RMSE <- mean(rf_reg_wkday$mse)
    best_mtry <- search_grid[i, ]$mtry
    best_ntree <- search_grid[i, ]$ntree
    
  }

  }

# Train the random forest model using the best 'mtry' and 'ntree'.
rf_reg_best_wkday <- randomForest(No_of_transaction~.,
                           df_weekday[train_wkday, ],
                           mtry = best_mtry,
                           ntree = best_ntree,
                           importance = TRUE
                           )

rf_reg_best_wkday

```

Then we train a regression trees model using boosting algorithm to compare with the random forest model constructed.
**Weekday**
```{r}
search_grid <- expand.grid(shrinkage = c(0.01, 0.001),
                           interaction.depth = c(4, 5),
                           n.minobsinnode = c(10, 100),
                           bag.fraction = c(0.5, 0.8),
                           optimal_trees = 0,
                           min_RMSE = 0)

for (i in seq(1, nrow(search_grid))){

  boost.fit_wkday <- gbm(No_of_transaction~. ,
                  data = df_weekday[train_wkday, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = search_grid[i, ]$shrinkage,
                  interaction.depth = search_grid[i, ]$interaction.depth,
                  n.minobsinnode = search_grid[i, ]$n.minobsinnode,
                  bag.fraction = search_grid[i, ]$bag.fraction,
                  )
  search_grid[i, ]$min_RMSE <- min(boost.fit_wkday$cv.error)
  search_grid[i, ]$optimal_trees = match(min(boost.fit_wkday$cv.error), boost.fit_wkday$cv.error)

}

# Train and fit this model with the best sets of hyperparameters:

boost_best_para_wkday <- 
  search_grid[search_grid$min_RMSE == min(search_grid$min_RMSE),]

boost.best_wkday <- gbm(No_of_transaction~. ,
                  data = df_weekday[train_wkday, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = boost_best_para_wkday$shrinkage,
                  interaction.depth = boost_best_para_wkday$interaction.depth,
                  n.minobsinnode = boost_best_para_wkday$n.minobsinnode,
                  bag.fraction = boost_best_para_wkday$bag.fraction,
                  )

boost.best_wkday

```

Then we compare the performance of both the random forest model and the trees model using boosting algorithm on the Weekday validation set.

```{r}
boost_pred_test_wkday <- predict(boost.best_wkday,
                           newdata = df_weekday[-train_wkday, ],
                           n.trees = 4000)
boost_RMSE_test_wkday <- caret::RMSE(boost_pred_test_wkday, df_weekday[-train_wkday, ]$No_of_transaction)

rf_pred_test_wkday <- predict(rf_reg_best_wkday,
                        newdata = df_weekday[-train_wkday, ])

rf_RMSE_test_wkday <- caret::RMSE(rf_pred_test_wkday, df_weekday[-train_wkday, ]$No_of_transaction)

print(paste0('RMSE of the boosting model on Weekday testing set:', round(boost_RMSE_test_wkday, 4)))
print(paste0('RMSE of the random forest model on Weekday testing set:', round(rf_RMSE_test_wkday, 4)))

```
**test error smaller than training error??** 

**Weekend**
```{r}
search_grid <- expand.grid(mtry = c(round(num_features/3/2),
                                    round(num_features/3),
                                    round(num_features/3*2)),
                           ntree = c(100, 500, 1000))

min_RMSE <- Inf
best_mtry <- 0
best_ntree <- 0

for (i in seq(1, nrow(search_grid))){
  rf_reg_wkend <- randomForest(No_of_transaction~.,
                           df_weekend[train_wkend, ],
                           mtry = search_grid[i, ]$mtry,
                           ntree = search_grid[i, ]$ntree,
                           importance = TRUE
                           )
  if (mean(rf_reg_wkend$mse) < min_RMSE) {
    min_RMSE <- mean(rf_reg_wkend$mse)
    best_mtry <- search_grid[i, ]$mtry
    best_ntree <- search_grid[i, ]$ntree
    
  }

  }

# Train the random forest model using the best 'mtry' and 'ntree'.
rf_reg_best_wkend <- randomForest(No_of_transaction~.,
                           df_weekend[train_wkend, ],
                           mtry = best_mtry,
                           ntree = best_ntree,
                           importance = TRUE
                           )

rf_reg_best_wkend

```


```{r}
search_grid <- expand.grid(shrinkage = c(0.01, 0.001),
                           interaction.depth = c(4, 5),
                           n.minobsinnode = c(10, 20),
                           bag.fraction = c(0.5, 0.8),
                           optimal_trees = 0,
                           min_RMSE = 0)

for (i in seq(1, nrow(search_grid))){

  boost.fit_wkend <- gbm(No_of_transaction~. ,
                  data = df_weekend[train_wkend, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = search_grid[i, ]$shrinkage,
                  interaction.depth = search_grid[i, ]$interaction.depth,
                  n.minobsinnode = search_grid[i, ]$n.minobsinnode,
                  bag.fraction = search_grid[i, ]$bag.fraction,
                  )
  search_grid[i, ]$min_RMSE <- min(boost.fit_wkend$cv.error)
  search_grid[i, ]$optimal_trees = match(min(boost.fit_wkend$cv.error), boost.fit_wkend$cv.error)

}

# Train and fit this model with the best sets of hyperparameters:

boost_best_para_wkend <- 
  search_grid[search_grid$min_RMSE == min(search_grid$min_RMSE),]

boost.best_wkend <- gbm(No_of_transaction~. ,
                  data = df_weekend[train_wkend, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = boost_best_para_wkend$shrinkage,
                  interaction.depth = boost_best_para_wkend$interaction.depth,
                  n.minobsinnode = boost_best_para_wkend$n.minobsinnode,
                  bag.fraction = boost_best_para_wkend$bag.fraction,
                  )

boost.best_wkend

```


```{r}
boost_pred_test_wkend <- predict(boost.best_wkend,
                           newdata = df_weekend[-train_wkend, ],
                           n.trees = 4000)
boost_RMSE_test_wkend <- caret::RMSE(boost_pred_test_wkend, df_weekend[-train_wkend, ]$No_of_transaction)

rf_pred_test_wkend <- predict(rf_reg_best_wkend,
                        newdata = df_weekend[-train_wkend, ])

rf_RMSE_test_wkend <- caret::RMSE(rf_pred_test_wkend, df_weekend[-train_wkend, ]$No_of_transaction)

print(paste0('RMSE of the boosting model on Weekend testing set:', round(boost_RMSE_test_wkend, 4)))
print(paste0('RMSE of the random forest model on Weekend testing set:', round(rf_RMSE_test_wkend, 4)))

```
**test error smaller than training error??** 


It seems that the random forest model selected performs better than the other model selected. For the random forest model, the RMSE on the training set is `r round(sqrt(mean(rf_reg_best$mse)), 4)`, which doesn't suggest the risk of overfitting.

Now that we have the regression model constructed, we would like to further understand the impact of different features.
**Weekday
```{r, fig.width=7,fig.width=7}
varImpPlot(rf_reg_best_wkday)

```

The most important factors are whether the time is in the rush hours - this is not surprising. What's interesting is that the weather related variables have similar level impact on node purity as the one of whether a day is weekend. We use partial dependence plot here to further understand the impact of weather related variables.

**Weekday**
```{r, fig.width=7,fig.width=7}
par(mfrow = c(2, 2))
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'wdir')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'wspd')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'rhum')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'temp')


```

From the partial dependence plot for Weekday data, we could learn that:

1. Temperature seems to have a U-shape effect on number of transactions. When the temperature is approximately below 2 Celsius, number of transactions decreases as temperature increases; when it is approximately above 2 Celsius, number of transactions increases as temperature increases. Potentially because during weekdays, there could be increasing number of transactions during rush hour, especially in the morning, when the temperature tends to be low. 

2. When there is the north wind or north-east wind (direction of wind between 350 and 56), the sales will be higher than the case when the wind is blowing from other directions.

3. Average-level humidity seems to have a positive effect on number of transactions; however, when humidity is close to or over 90, (**indicator of raining?**) there is a sharp decrease, showing a negative effect on number of transactions. 

4. Stronger wind has negative impact on the sales

To sum up, the most important variable in forecasting the number of transactions at a given time is whether it is during the rush hour. Also, different weather related variables have different impact on the number of transaction - so watch the weather forecast and be prepared.

**Weekday**
```{r, fig.width=7,fig.width=7}
par(mfrow = c(2, 2))
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'wdir')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'wspd')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'rhum')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'temp')


```
From the partial dependence plot for Weekend data, we could learn that:

1. Interestingly, in contrast to the data in weekdays, temperature no longer has a U-shape effect; instead, overall, as temperature increases, number of transactions increases. 

2. In weekends, when there is north wind or north-west wind (270-350), there is a huge increase in number of transactions. 

3. In weekends, as humidity increases, number of transactions decreases overall. 

4. Just as weekdays, wind speed has a negative effect on number of transactions.


Maybe weather-related variables are more meaningful for weekend data, since people have to go to work in weekdays regardless of the weather? 

##4.3 Market Basket Analysis

The objective of this analysis is to identify possible association rules. For example, assuming we identify from the data that people who buy cake tend to buy cookie at the same time, then the bakery could consider to place these two item closer in the shop in order to boost the sales.

We have already re-grouped different items, so for this analysis, we will start from looking at the association rules among item types.

```{r}
transactions_Item_Type <- as(df_merged_mba_item_type$basket, "transactions")
#inspect(transactions_Item_Type[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS
rules_Item_Type <- apriori(transactions_Item_Type, 
                 parameter = list(support = 0.05, 
                                  confidence = 0.025,
                                  minlen = 2))
# Remove redundant rules
rules_Item_Type <- rules_Item_Type[!is.redundant(rules_Item_Type)]

# Reorder by Lift and display
rules_Item_Type_dt <- data.table( lhs = labels( lhs(rules_Item_Type) ), 
                        rhs = labels( rhs(rules_Item_Type) ), 
                        quality(rules_Item_Type) )[ order(-lift), ]
head(rules_Item_Type_dt, 10)
#rules_Item_Type_dt

```

From the table we observe that the highest lift of all rules is still smaller than 1. That says, no valid association rules identified among the item types.

Then we focus our market basket analysis on the detail list of items, instead of the category.
**Weekday**
```{r}
transactions_Item <- as(mba_wkday$basket, "transactions")
#inspect(transactions_Item[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS
rules_Item <- apriori(transactions_Item, 
                 parameter = list(support = 0.01, 
                                  confidence = 0.25,
                                  minlen = 2)
                 )

# Remove redundant rules
rules_Item <- rules_Item[!is.redundant(rules_Item)]

# Reorder by Lift and display
rules_Item_dt_wkday <- data.table( lhs = labels( lhs(rules_Item) ), 
                        rhs = labels( rhs(rules_Item) ), 
                        quality(rules_Item) )[ order(-lift), ]
rules_Item_dt_wkday%>%filter(lift>1)
#rules_Item_dt


```

Few observations could be made from the table:
During weekdays
1. People who buy coke tends to buy sandwich at the same time

2. People who buy cookies or cake tends to buy coffee and juice at the same time (buying two drinks together?-maybe sharing with friends?)

3. The top 5 things that people who buy tea also buy are soup (??), or cake, or brownie, or sandwich, or Alfajores.

4. In contrast, the top 5 things that people who buy coffee also buy are toast, or salad, or cake and cookies, or cake and hot chocolates

5. People who buy bread tends to buy pastry at the same time 

Based on the above, maybe we could recommend the bakery, for weekdays, to create bundles sales for  coke & sandwiches, tea & soup/brownie/Alfajores, coffee & toast/salad/cookies/cakes. For cakes, it seems like it tends to go with two drinks, so there could be combo that includes one cake and two drinks. 


**Weekend**
```{r}
transactions_Item <- as(mba_wkend$basket, "transactions")
#inspect(transactions_Item[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS
rules_Item <- apriori(transactions_Item, 
                 parameter = list(support = 0.01, 
                                  confidence = 0.25,
                                  minlen = 2)
                 )

# Remove redundant rules
rules_Item <- rules_Item[!is.redundant(rules_Item)]

# Reorder by Lift and display
rules_Item_dt_wkend <- data.table( lhs = labels( lhs(rules_Item) ), 
                        rhs = labels( rhs(rules_Item) ), 
                        quality(rules_Item) )[ order(-lift), ]
rules_Item_dt_wkend%>%filter(lift>1)
#rules_Item_dt


```
In weekends:

1. In contrast to weekdays, the top association is farm house and medialuna. (not sure farm house?)

2. In contrast to weekdays, cake is often bought with coffee and tea, or coffee and hot chocolate, or bread and tea, or tea only, or hot chocolate only. 

3. People who buy tea tends to buy scones at the same time (why not in weekdays?)

4. The top 5 things that people who buy coffee also buy are hot chocolate and pastry, or spanish brunch (as supported in the previous analysis), or cake and sandwich, or the Nomad, or pastry and tea.

5. People who buy bread tends to buy jam at the same time 


Based on the above, we could recommend the bakery to keep the combo for cake with two drinks in weekends. For weekends only, maybe include some bundle sales for farm house & medialuna, tea & scones, bread & jam, coffee & spanish brunch


***Updates till here only***

```{r}

p1<-df%>%filter(Item=='Toast')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Toast by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p2<-df%>%filter(Item=='Spanish Brunch')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Spanish Brunch by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p3<-df%>%filter(Item=='Medialuna')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Medialuna by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p4<-df%>%filter(Item=='Pastry')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Pastry by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)


grid.arrange(p1, p2, p3, p4,
             nrow=2,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours",
                            gp=gpar(fontsize=14,font=3)))

```

It turns out that Spanish brunch is sold more during lunch hour. This indeed brings another question - is people buying more Spanish brunch during the workdays or during the weekends?


```{r}
df%>%filter(Item=='Spanish Brunch')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Weekday), y = Avg_No_of_transaction, fill = as.factor(Weekday))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Spanish Brunch by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)
```

It could be observed that Spanish brunch is sold more during the weekends. In a nutshell, there are two suggestions to the Bakery from analyzing the association rules:

1. Promote bundles of toast and coffee, medialuna and coffee, and pastry and coffee, boosting the sales of both.

2. Promote bundles of Spanish brunch and coffee during the weekends, trying to boost the sales of both.

##4.4 Clustering

XXXXXX

```{r}

fviz_nbclust(
  df_merged_cluster,
  kmeans,
  k.max = 10,
  method = "wss"
)


fviz_nbclust(
  df_merged_cluster,
  kmeans,
  k.max = 10,
  method = "silhouette"
)

```

Both figures suggest that the best K should be 2. So we will start with k = 2.

```{r}

km_out_2 <- kmeans(df_merged_cluster, 2, nstart = 25)

fviz_cluster(km_out_2, df_merged_cluster, geom = 'point', ellipse.type = 'norm',
             main = 'K-Means Clustering Results with K=2')

```

Apparently, PCA might not be the ideal way to extract the features for visualization. The first and second principle components together could only explain 30% of variance.

```{r}
library(Rtsne)

df_merged_cluster <- distinct(df_merged_cluster)

tsne_out <- Rtsne(df_merged_cluster,
                  dims = 2,
                  initial_dims = 30,
                  perplexity = 150)
plot(tsne_out$Y)

```

```{r}
plot(tsne_out$Y)
```

#5 Conclusion

#6 xxx
