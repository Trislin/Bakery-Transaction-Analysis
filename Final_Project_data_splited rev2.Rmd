---
title: "Final Project - Bakery Transaction"
author: "Shuoqi Zhang, Yijing(Trista) Lin"
date: "11/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Include the libraries we are going to need here
library(caret)
library(ggplot2)
library(plyr)
#library(tidyr)
library(tidyverse)
library(ggcorrplot)
library(knitr)
library(splines)
library(glmnet)
library(gridExtra)
library(grid)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(arules) # for Association Rules analysis
library(data.table)
library(gridExtra)
library(tree)
library(factoextra)
library(cluster)
library(arulesViz)

nb.cols <- 18
mycolors <- colorRampPalette(brewer.pal(8, "Blues"))(nb.cols)
cookiecol<-c('#ad6a1d','#9a5327','#cc8d4a','#4e1703','#ecc78d','#2e0a05','#d0dfe4','#33575b', '#173742')

```

# 1 Background

The main dataset is the transaction record of a bakery (https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery). Though not specified in the description of the dataset, it is implied that this bakery is located in the old town of Edinburgh, UK. The dataset is downloaded from kaggle and stored under the same path as this R markdown file.

In order to supplement the dataset with more features, we extract the historical weather records from Meteostat (https://dev.meteostat.net/python/hourly.html#response-parameters). We use the weather data recorded by a weather station in Edinburgh Airport, which is about 8 miles away from the Edinburgh old town. The dataset is downloaded and stored under the same path of this R markdown file.

# 2 Questions To Be Answered

Combining the bakery transaction record and the historical weather record, there are few questions we could further explore, such as:

1. Knowing the date, hour, and weather forecast, to predict the number of transactions during that specific hour.

2. Which items tend to be purchased together?

3. Is there any cluster of the transactions?


# 3 Data Cleaning and Wrangling

## 3.1 Read The Dataset

Firstly we will read the dataset of the bakery transaction record, and the hourly weather record in Edinburgh.

```{r}
df_ori <- read.csv(file = 'BreadBasket_DMS.csv',
               header = TRUE,
               encoding = 'utf-8')

df_weather_ori <- read.csv(file = 'Edinburgh_weather_hourly.csv',
                       header = TRUE,
                       encoding = 'utf-8')

```

Then we display the first few rows of each data set.

```{r}
head(df_ori)
head(df_weather_ori)
```
We will create the keys in order to merge two datasets.

```{r}
df <- df_ori
df_weather <- df_weather_ori
df$Date <- as.Date(df$Date, "%Y-%m-%d")
df$Hour <- as.numeric(substr(df$Time, 1, 2))
df$key <- paste(as.character(df$Date), "@", as.character(df$Hour))
df_weather <- df_weather %>% separate(time, c("Date", "Hour_Minute"), " ")
df_weather <- df_weather %>% separate(Hour_Minute, c("Hour", "Minute"), ":")
df_weather$Date <- as.Date(df_weather$Date, "%m/%d/%Y")
df_weather$Hour <- as.numeric(df_weather$Hour)
df_weather$key <- paste(as.character(df_weather$Date), "@", as.character(df_weather$Hour))

```

## 3.2 The Bakery Transaction Record

### 3.2.1 NA Values and Duplicated Rows

Now we will deal with the data type and missing values, if any. We will start with the bakery transaction record database.

```{r}
summary(df)
```

There is no NAs. Then we will verify whether there are duplicated rows in the dataset

```{r}
dim(df[duplicated(df), ])[1]
```

There are duplicated rows. Considering that our analysis will not focus on the quantities of item sold, it is OK to remove those duplicated rows.

```{r}
df <- distinct(df)
```

### 3.2.2 Weekday vs. Weekend

Given the nature of the bakery business, we may expect different behaviors during weekdays and weekends.

```{r}
df$Weekday <- weekdays(df$Date, abbreviate = TRUE)

df$Weekday <- factor(df$Weekday, 
                        levels = c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'))


res <- ddply(df, ~Weekday, summarise, No_of_transaction = length(unique(Transaction)))

ggplot(data = res, mapping = aes(x = Weekday, y = No_of_transaction, fill = Weekday)) +
  geom_bar(stat = 'identity',width=0.7,alpha=0.8) +
  labs(title = 'No. of Transactions by Weekdays', x = 'Weekdays', y = 'Number of Transactions') +
  scale_fill_brewer(palette = "Blues") +
  theme_minimal()



```

As expected, there are more transactions on Saturday. However, the number of transactions on Sunday seem to be low. To further understand what happened, we will look at the number of transactions by hour by weekdays.


```{r}
res <- ddply(df, .(Weekday, Hour), summarise, No_of_transaction = length(unique(Transaction)))

ggplot(data = res, mapping = aes(x = as.factor(Hour), y = No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = "identity",alpha=0.8) + facet_wrap(~ Weekday) +
  labs(title = 'No. of Transactions by Hour by Weekdays', x = 'Hours', y = 'Number of Transactions') +
  scale_fill_manual(values = mycolors) +
  theme_minimal() + 
  theme(legend.position = "none",
axis.text = element_text(size=8),
)

```

Looking at the distribution of transactions by hours, we noticed that the trend for Saturday and Sunday are similar and are different from the ones of the other days. Therefore, it makes sense to group Saturday and Sunday together, though Sunday has less transactions comparing with Saturday. This also implies that we may want to split the dataset into weekdays data and weekend data to perform further analysis.

```{r}
df$Weekend <- mapvalues(df$Weekday,
                        from = c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'),
                        to = c(0, 0, 0, 0, 0, 1, 1))


```

### 3.2.3 Hours of The Day

From the figure presented in 3.2.2, we noticed that there are few transactions associated with abnormal hours, such as 1 am and 11 pm.

```{r}
res <- ddply(df, ~Hour, summarise, No_of_transaction = length(unique(Transaction)))
ggplot(data = res, mapping = aes(x = as.factor(Hour), y = No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'No. of Transactions by Hour', x = 'Hours', y = 'Number of Transactions') +
  scale_fill_manual(values = mycolors) +
  theme_minimal() +
  theme(legend.position = "none")


```

Considering the small amount of transactions associated with abnormal hours, we will drop the rows whose hours is outside a normal business operating time. In other words, we will drop rows whose hour is 1, 21, 22, or 23 from our dataset.

```{r}
df <- df%>%filter(Hour<21&Hour>1)
```

Another observation is that the amount of transaction within an hour varies by the time of the day. Here we split the day into two segments: rush hours from 9 to 15 and non-rush hours for the rest of the day. Such split makes sense practically as the period from 9 to 15 covers breakfast, lunch, and coffee or tea time in the afternoon.

```{r}
df$Rush_hours <- mapvalues(df$Hour,
                        from = c(7, 8,
                                 9, 10, 11, 12, 13, 14, 15,
                                 16, 17, 18, 19, 20),
                        to = c(0, 0,
                               1, 1, 1, 1, 1, 1, 1,
                               0, 0, 0, 0, 0))
df$Rush_hours<-as.factor(df$Rush_hours)
```

### 3.2.4 Holidays
We also want to take a look at sales during holidays. We will focus on two holidays, Christmas and New Year. 
0 = non-holiday
1 = Christmas|New Year ï¼ˆ12/24/2016 - 1/1/2017)
Bakery did not open on 12/25/2016, 12/26/2016, 1/1/2017

```{r}
df$Holiday<-0
df$Holiday[df$Date>'2016-12-23'&df$Date<'2017-01-02']<-1
hol<-df%>%group_by(Holiday)%>%summarize(mean_transaction = length(unique(Transaction))/length(unique(Date)))
hol%>%ggplot(aes(as.factor(Holiday),mean_transaction, fill = as.factor(Holiday),label = round(mean_transaction,2))) +
  labs(title = 'Average No. of Transactions by Holiday', x = 'Holiday', y = 'Average Number of Transactions') +
  geom_bar(stat = 'identity', width = 0.5,alpha=0.8) +
  geom_text(vjust = -0.5) +
  scale_fill_manual(values = cookiecol[c(5,7)]) +
  theme_minimal()
df$Holiday<-as.factor(df$Holiday)

```

Indeed, the average number of transaction on holidays is 12% less than the one of non-holidays. 


### 3.2.5 List of Purchased Items

Then we will focus on the "Item" column to understand what are included.

```{r}
Item_tb <- table(df$Item)
sort(Item_tb, decreasing = TRUE)
```

This list of itme seems to be inconsistent and confusing. For example, there are items named "NONE". Also, Brownie is separated from Cakes, Baguette is not considered as Bread, and Medialuna is treated differently from Pastry.

The item type "Adjustment" and "None" are probably introduced by the transaction tracking system or the cashier. In other words, there is no real purchase behind each of them. So we will drop them from the dataset. 
Then, the 'Item_Type' column is reorganized and coded as following: 

Bread = 1

Cookies = 2

Cake|Pastry|Sweets = 3

Coffee = 4

Tea = 5

Hot chocolate|Smoothie|Juice = 6

Other beverage = 7

Meal = 8

Other = 9 

*Ambiguous items are coded as 'Other'

```{r}
df <- df %>% filter(Item!='NONE' & Item!='Adjustment')
df$Item_Type <- 9
df$Item_Type[df$Item%in%c('Bread', 'Farm House', 'Toast','Baguette','Focaccia')]<-1
df$Item_Type[df$Item == 'Cookies']<-2
df$Item_Type[df$Item%in%c('Cake','Pastry','Medialuna','Brownie','Muffin','Alfajores','Scone','Scandinavian','Truffles','Tiffin','Fudge','Jammie Dodgers','Bakewell','Tartine','Vegan mincepie')]<-3
df$Item_Type[df$Item == 'Coffee']<-4
df$Item_Type[df$Item == 'Tea']<-5
df$Item_Type[df$Item%in%c('Hot chocolate', 'Juice', 'Smoothies')]<-6
df$Item_Type[df$Item%in%c('Mineral water', 'Coke')]<-7
df$Item_Type[df$Item%in%c('Sandwich', 'Soup', 'Spanish Brunch', 'Chicken Stew', 'Salad','Frittata')]<-8
df$Item_Type<-as.factor(df$Item_Type)
df%>%group_by(Item_Type)%>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Item_Type,y=Count,fill=Item_Type,label = Count)) +
  geom_bar(stat="identity", width=0.7,alpha=0.7) +
  theme_minimal() + 
  scale_fill_manual(values = cookiecol,labels = c("1:Bread", "2:Cookies", "3:Cake|Pastry|Sweets",'4:Coffee','5:Tea','6:Hot chocolate|Smoothie|Juice','7:Other beverage','8:Meal','9:Other')) +
  geom_text(vjust = -0.5,size=3) + 
  labs(title = 'Item Frequency in Unique Transactions', x = 'Item Type', y = 'Number of Transactions') 
```

From the Figure above, cakes/pastries/sweets are the most popular items in the bakery, followed by Coffee and Bread.

We also want to see if the transaction of each item varies by the hours in the day. We decided to focus on bread, cookies, cake/pastry/sweets, coffee and tea. 

```{r}

bread<-df%>%filter(Item_Type==1)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  facet_wrap(~ Item_Type) +
  labs(title = 'Average Number of Transactions of Bread by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)


cookie<-df%>%filter(Item_Type==2)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cookies by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

pastry<-df%>%filter(Item_Type==3)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cakes/Pastries/Sweets by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

coffee<-df%>%filter(Item_Type==4)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Coffee by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

tea<-df%>%filter(Item_Type==5)%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(df$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Tea by Hour', x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

grid.arrange(bread,cookie,pastry,coffee,tea, nrow=3,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours: Bread,Cookies,Pastries,Coffee, and Tea",gp=gpar(fontsize=14,font=3)))



```

The Figure above shows that bread and coffee on average tend to be sold more in the morning (~11am) while tea tends to be sold more in the afternoon. Transaction frequencies of cookies and cakes/pastries/sweet have peaks in both morning and afternoon. 


## 3.3 The Edinburgh Weather Record

Let's start with the high level summary of the dataset.

```{r}
summary(df_weather)
```

There are NA values in some of the columns. The columns "prcp", "snow", "wpgt" and "tsun" contain only NA values, so we will drop them.

The website of Meteostat gives clear explanation of each column (https://dev.meteostat.net/python/hourly.html#response-parameters):

1. station: The Meteo ID of the weather station

2. temp: The air temperature in C

3. dwpt: The dew point in C

4. rhum: The relative humidy in percent

5. wdir: The average wind direction in degrees

6. wspd: The average wind speed in km/h

7. pres: The average sea-level air pressure in hPa

8. coco: The weather condition code (https://dev.meteostat.net/docs/formats.html#weather-condition-codes). Only significant weather events are reported here.

Based on the description of each columns, we could drop the "station" column, as it remains the same for all the observations. We could also drop the "dwpt" column, as we already include the "temp" in our features. Another variable to drop is "pres", as the air pressure is difficult to interpret for people without the background in Meteorology. For the missing values in "coco", we will fill 0, as it stands for non-significant weather events.

We will also remove the "Minute" column as it is always 00.

```{r}
df_weather <- subset(df_weather,
                     select = -c(Minute, prcp, snow, wpgt, tsun, station, dwpt, pres))
```

For the missing values in "wdir" and "wspd", considering the consistency of weather conditions, we will use the value of the hour right after.

```{r}
df_weather$coco[is.na(df_weather$coco)] <- 0

df_weather <- df_weather %>% tidyr::fill(wdir, .direction = "up")
df_weather <- df_weather %>% tidyr::fill(wspd, .direction = "up")


```


## 3.4 The Combined Dataset

We now will merge the two datasets based on the pre-defined keys.

```{r}
df_merged <- merge(x = df, y = df_weather, by = 'key', all.x = TRUE)
df_merged$Date <- df_merged$Date.x
df_merged$Hour <- df_merged$Hour.x
df_merged <- subset(df_merged,
                    select = -c(key, Time, Date.y, Hour.y, Date.x, Hour.x))
df_merged$coco<-as.factor(df_merged$coco)
```


```{r}

df_merged_feature <- subset(df_merged, select = -c(Transaction, Item, Weekday,Weekend, Item_Type, Date,Holiday,coco,Rush_hours))
#Calculate correlation here
corr <- round(cor(df_merged_feature), digits = 2)

#Use ggcorrplot to graph correlation. Only plot the lower triangle of the correlation matrix.
ggcorrplot(corr, type = "lower", 
           ggtheme = ggplot2::theme_minimal,
           lab = TRUE,
           colors = c(cookiecol[5],'white',cookiecol[7]))
```

The correlation table suggests possible correlation between the temperature and the humidity, and between the temperature and the speed of wind.

## 3.5 Dataset Construction

### 3.5.1 Predict Transactions By Day of week, Hour, and Weather Information

```{r}
# To build the dataset for the regression problem
# Keep Hour as a feature
df_merged_reg<-df_merged%>%
  group_by(Date, Hour)%>%
  mutate(No_of_transaction = length(unique(Transaction)))%>%
  ungroup()%>%
  #select(-c(Transaction, Item, Item_Type,Date,Hour))%>%
  select(-c(Transaction, Item, Item_Type,Date))%>%
  unique()%>%
  data.frame()

# Keep Weekday as a feature
df_weekday<-df_merged_reg%>%
  filter(Weekend==0)%>%
  #select(-Weekday,-Weekend)%>%
  select(-Weekend)%>%
  data.frame()

summary(df_weekday)

# Keep Weekday as a feature
df_weekend<-df_merged_reg%>%
  filter(Weekend==1)%>%
  #select(-Weekday,-Weekend)%>%
  select(-Weekend)%>%
  data.frame()

summary(df_weekend)
```

Next, we plot scatter plots of number of transaction against different weather features, respectively.

```{r}
# scatter plots of number of transaction against different weather features, respectively
temp<-df_merged_reg%>%ggplot(aes(temp,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Temperature in Celcius', y='Number of Transaction') + 
  theme_minimal()

ws<-df_merged_reg%>%ggplot(aes(wspd,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Wind Speed', y='Number of Transaction') + 
  theme_minimal()

hum<-df_merged_reg%>%ggplot(aes(rhum,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Humidity', y='Number of Transaction') + 
  theme_minimal()

wcc<-df_merged_reg%>%ggplot(aes(coco,No_of_transaction)) + 
  geom_jitter(alpha = 0.7, size=0.7, colour = cookiecol[3]) +
  labs(x='Weather Condition Code', y='Number of Transaction') + 
  theme_minimal()

grid.arrange(temp,ws,hum,wcc, nrow = 2)



```


### 3.5.2 Market Basket Analysis

Here we will construct the dataset to perform market basket analysis at a later stage.

```{r}
df_merged_mba <- df_merged

# map back the name of Item type

df_merged_mba$Item_Type_Name[df$Item_Type == 1] <- 'Bread'
df_merged_mba$Item_Type_Name[df$Item_Type == 2] <- 'Cookies'
df_merged_mba$Item_Type_Name[df$Item_Type == 3] <- 'Cake|Pastry|Sweets'
df_merged_mba$Item_Type_Name[df$Item_Type == 4] <- 'Coffee'
df_merged_mba$Item_Type_Name[df$Item_Type == 5] <- 'Tea'#'Other beverage'#'Tea'
df_merged_mba$Item_Type_Name[df$Item_Type == 6] <- 'Hot chocolate|Smoothie|Juice'#'Other beverage'
df_merged_mba$Item_Type_Name[df$Item_Type == 7] <- 'Other beverage'
df_merged_mba$Item_Type_Name[df$Item_Type == 8] <- 'Meal'
df_merged_mba$Item_Type_Name[df$Item_Type == 9] <- 'Other'


df_merged_mba_item_type_temp <- subset(df_merged_mba,
                                       select = c(Transaction, Item_Type_Name, Weekend))

df_merged_mba_item_type_temp <- distinct(df_merged_mba_item_type_temp)

# Remove the transactions including only one item type
df_merged_mba_item_type_temp <-
  df_merged_mba_item_type_temp %>%
  group_by(Transaction) %>%
  mutate(freq = n()) %>%
  data.frame()

df_merged_mba_item_type_temp <- df_merged_mba_item_type_temp[df_merged_mba_item_type_temp$freq > 1, ]


# MBA on category of items
df_merged_mba_item_type <- df_merged_mba_item_type_temp %>%
  group_by(Transaction) %>%
  summarise(basket = as.vector(list(Item_Type_Name)))

# MBA on items
df_merged_mba_item <- df_merged_mba %>%
  group_by(Transaction) %>%
  mutate(basket = as.vector(list(Item)))%>%ungroup()%>%data.frame()

mba_wkday<-df_merged_mba_item%>%
  filter(Weekend==0)%>%
  select(-Weekend)

mba_wkend<-df_merged_mba_item%>%
  filter(Weekend==1)%>%
  select(-Weekend)


```

### 3.5.3 Study The Clusters of Transactions

Here we will add the number of type of items for each transaction.

```{r}

df_merged_cluster<-df_merged%>%group_by(Transaction)%>%
  mutate(No_Item_Type = length(unique(Item)))%>%
  ungroup()%>%
  select(-Transaction,-Item, -Item_Type, -Weekday,-Date)%>%
  unique()%>%
  data.frame()

df_merged_cluster[,c(2:10)]<-sapply(df_merged_cluster[,c(2:10)], as.numeric)

cluster_wkday<-df_merged_cluster%>%filter(Weekend==0)%>%select(-Weekend)%>%data.frame()
cluster_wkend<-df_merged_cluster%>%filter(Weekend==1)%>%select(-Weekend)%>%data.frame()


summary(cluster_wkday)
summary(cluster_wkend)
```



# 4 Modeling and Analysis

## 4.1 Predicting The Number of Transactions

The task of this analysis is to predict the number of transactions during that specific hour, knowing the date, hour, and weather forecast. More importantly, from the model created, we expert to further understand how the sales is driven by external variables, such as weather, day of the week, time of the day and so on.

Firstly, we will split the dataset into training set and validation set.

```{r}
set.seed(0)

train_wkday<-sample(1:nrow(df_weekday), round(0.8*dim(df_weekday)[1]))
train_wkend <- sample(1:nrow(df_weekend), round(0.8*dim(df_weekend)[1]))
num_features <- dim(df_weekday)[2]

```

### 4.1.1 Lasso Regression Model

We will start with Lasso regression model.
**Weekday**
```{r}
x_wkday <- model.matrix(No_of_transaction~.,df_weekday)[,-1]
y_wkday <- df_weekday$No_of_transaction

y.test_wkday <- y_wkday[-train_wkday]

grid=10^seq(10,-2, length =100)
```

```{r}
lasso.mod=glmnet(x_wkday[train_wkday,],y_wkday[train_wkday],alpha=1,lambda=grid)
plot(lasso.mod)
```

```{r}
cv.out=cv.glmnet(x_wkday[train_wkday,],y_wkday[train_wkday],alpha=1)
plot(cv.out)

```

Then we run the lasso model on the test set to see the RMSE.

```{r}
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_wkday[-train_wkday,])
#RMSE
lasso_RMSE_wkday <- sqrt(mean((lasso.pred-y.test_wkday)^2))
lasso_RMSE_wkday

```

```{r}
out <- glmnet(x_wkday,y_wkday,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:8,]
lasso.coef
```

```{r}
lasso.coef[lasso.coef!=0]
```

**Weekend**
```{r}
x_wkend <- model.matrix(No_of_transaction~.,df_weekend)[,-1]
y_wkend <- df_weekend$No_of_transaction

y.test_wkend <- y_wkend[-train_wkend]

grid=10^seq(10,-2, length =100)
```

```{r}
lasso.mod=glmnet(x_wkend[train_wkend,],y_wkend[train_wkend],alpha=1,lambda=grid)
plot(lasso.mod)

```

```{r}
cv.out=cv.glmnet(x_wkend[train_wkend,],y_wkend[train_wkend],alpha=1)
plot(cv.out)
```

The test set RMSE of the model chosen by lasso is

```{r}
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_wkend[-train_wkend,])
#RMSE
lasso_RMSE_wkend <- sqrt(mean((lasso.pred-y.test_wkend)^2))
lasso_RMSE_wkend

```

```{r}
out <- glmnet(x_wkend,y_wkend,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:8,]
lasso.coef
```

```{r}
lasso.coef[lasso.coef!=0]
```

### 4.1.2 Random Forest Modle and GBM Modle

***Random Forest***
Secondly, we try another model - random forest regression model. We will try several different sets of hyperparameters, including 'mtry' and 'ntree' to figure out the best random forest regression model as baseline.
**Weekday**
```{r}
search_grid <- expand.grid(mtry = c(round(num_features/3/2),
                                    round(num_features/3),
                                    round(num_features/3*2)),
                           ntree = c(100, 500, 1000))

min_RMSE <- Inf
best_mtry <- 0
best_ntree <- 0

for (i in seq(1, nrow(search_grid))){
  rf_reg_wkday <- randomForest(No_of_transaction~.,
                           df_weekday[train_wkday, ],
                           mtry = search_grid[i, ]$mtry,
                           ntree = search_grid[i, ]$ntree,
                           importance = TRUE
                           )
  if (mean(rf_reg_wkday$mse) < min_RMSE) {
    min_RMSE <- mean(rf_reg_wkday$mse)
    best_mtry <- search_grid[i, ]$mtry
    best_ntree <- search_grid[i, ]$ntree
    
  }

  }

# Train the random forest model using the best 'mtry' and 'ntree'.
rf_reg_best_wkday <- randomForest(No_of_transaction~.,
                           df_weekday[train_wkday, ],
                           mtry = best_mtry,
                           ntree = best_ntree,
                           importance = TRUE
                           )

rf_reg_best_wkday

```

Then we train a regression trees model using boosting algorithm to compare with the random forest model constructed.
**Weekday**
```{r}
search_grid <- expand.grid(shrinkage = c(0.01, 0.001),
                           interaction.depth = c(4, 5),
                           n.minobsinnode = c(10, 100),
                           bag.fraction = c(0.5, 0.8),
                           optimal_trees = 0,
                           min_RMSE = 0)

for (i in seq(1, nrow(search_grid))){

  boost.fit_wkday <- gbm(No_of_transaction~. ,
                  data = df_weekday[train_wkday, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = search_grid[i, ]$shrinkage,
                  interaction.depth = search_grid[i, ]$interaction.depth,
                  n.minobsinnode = search_grid[i, ]$n.minobsinnode,
                  bag.fraction = search_grid[i, ]$bag.fraction,
                  )
  search_grid[i, ]$min_RMSE <- min(boost.fit_wkday$cv.error)
  search_grid[i, ]$optimal_trees = match(min(boost.fit_wkday$cv.error), boost.fit_wkday$cv.error)

}

# Train and fit this model with the best sets of hyperparameters:

boost_best_para_wkday <- 
  search_grid[search_grid$min_RMSE == min(search_grid$min_RMSE),]

boost.best_wkday <- gbm(No_of_transaction~. ,
                  data = df_weekday[train_wkday, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = boost_best_para_wkday$shrinkage,
                  interaction.depth = boost_best_para_wkday$interaction.depth,
                  n.minobsinnode = boost_best_para_wkday$n.minobsinnode,
                  bag.fraction = boost_best_para_wkday$bag.fraction,
                  )

boost.best_wkday

```

Then we compare the performance of both the random forest model and the trees model using boosting algorithm on the Weekday validation set.

```{r}
boost_pred_test_wkday <- predict(boost.best_wkday,
                           newdata = df_weekday[-train_wkday, ],
                           n.trees = 4000)
boost_RMSE_test_wkday <- caret::RMSE(boost_pred_test_wkday, df_weekday[-train_wkday, ]$No_of_transaction)

rf_pred_test_wkday <- predict(rf_reg_best_wkday,
                        newdata = df_weekday[-train_wkday, ])

rf_RMSE_test_wkday <- caret::RMSE(rf_pred_test_wkday, df_weekday[-train_wkday, ]$No_of_transaction)

print(paste0('RMSE of the boosting model on Weekday testing set:', round(boost_RMSE_test_wkday, 4)))
print(paste0('RMSE of the random forest model on Weekday testing set:', round(rf_RMSE_test_wkday, 4)))
print(paste0('RMSE of the lasso regression model on Weekday testing set:', round(lasso_RMSE_wkday, 4)))

```

For weekdays data, it turns out that the random forest model performs the best on the test set in term of RMSE. So we will choose the random forest model to explain how different features impact the number of transactions in a certain hour during a weekday. For the random forest model, the RMSE on the training set is `r round(sqrt(mean(rf_reg_best_wkday$mse)), 4)`, which doesn't suggest the risk of overfitting. However, one thing worth-noting is that our random forest could only explain 34% of variance. This is probably due to the very limited amount of features we have in our original dataset. 


**Weekend**
```{r}
search_grid <- expand.grid(mtry = c(round(num_features/3/2),
                                    round(num_features/3),
                                    round(num_features/3*2)),
                           ntree = c(100, 500, 1000))

min_RMSE <- Inf
best_mtry <- 0
best_ntree <- 0

for (i in seq(1, nrow(search_grid))){
  rf_reg_wkend <- randomForest(No_of_transaction~.,
                           df_weekend[train_wkend, ],
                           mtry = search_grid[i, ]$mtry,
                           ntree = search_grid[i, ]$ntree,
                           importance = TRUE
                           )
  if (mean(rf_reg_wkend$mse) < min_RMSE) {
    min_RMSE <- mean(rf_reg_wkend$mse)
    best_mtry <- search_grid[i, ]$mtry
    best_ntree <- search_grid[i, ]$ntree
    
  }

  }

# Train the random forest model using the best 'mtry' and 'ntree'.
rf_reg_best_wkend <- randomForest(No_of_transaction~.,
                           df_weekend[train_wkend, ],
                           mtry = best_mtry,
                           ntree = best_ntree,
                           importance = TRUE
                           )

rf_reg_best_wkend

```


```{r}
search_grid <- expand.grid(shrinkage = c(0.01, 0.001),
                           interaction.depth = c(4, 5),
                           n.minobsinnode = c(10, 20),
                           bag.fraction = c(0.5, 0.8),
                           optimal_trees = 0,
                           min_RMSE = 0)

for (i in seq(1, nrow(search_grid))){

  boost.fit_wkend <- gbm(No_of_transaction~. ,
                  data = df_weekend[train_wkend, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = search_grid[i, ]$shrinkage,
                  interaction.depth = search_grid[i, ]$interaction.depth,
                  n.minobsinnode = search_grid[i, ]$n.minobsinnode,
                  bag.fraction = search_grid[i, ]$bag.fraction,
                  )
  search_grid[i, ]$min_RMSE <- min(boost.fit_wkend$cv.error)
  search_grid[i, ]$optimal_trees = match(min(boost.fit_wkend$cv.error), boost.fit_wkend$cv.error)

}

# Train and fit this model with the best sets of hyperparameters:

boost_best_para_wkend <- 
  search_grid[search_grid$min_RMSE == min(search_grid$min_RMSE),]

boost.best_wkend <- gbm(No_of_transaction~. ,
                  data = df_weekend[train_wkend, ], 
                  distribution = "gaussian",
                  cv.folds = 10,
                  n.trees = 4000,
                  train.fraction = 0.75,
                  n.cores = NULL,
                  verbose = FALSE,
                  shrinkage = boost_best_para_wkend$shrinkage,
                  interaction.depth = boost_best_para_wkend$interaction.depth,
                  n.minobsinnode = boost_best_para_wkend$n.minobsinnode,
                  bag.fraction = boost_best_para_wkend$bag.fraction,
                  )

boost.best_wkend

```


```{r}
boost_pred_test_wkend <- predict(boost.best_wkend,
                           newdata = df_weekend[-train_wkend, ],
                           n.trees = 4000)
boost_RMSE_test_wkend <- caret::RMSE(boost_pred_test_wkend, df_weekend[-train_wkend, ]$No_of_transaction)

rf_pred_test_wkend <- predict(rf_reg_best_wkend,
                        newdata = df_weekend[-train_wkend, ])

rf_RMSE_test_wkend <- caret::RMSE(rf_pred_test_wkend, df_weekend[-train_wkend, ]$No_of_transaction)

print(paste0('RMSE of the boosting model on Weekend testing set:', round(boost_RMSE_test_wkend, 4)))
print(paste0('RMSE of the random forest model on Weekend testing set:', round(rf_RMSE_test_wkend, 4)))
print(paste0('RMSE of the lasso regression model on Weekend testing set:', round(lasso_RMSE_wkend, 4)))

```


Same as the case of weekday data, for weekend data, it seems that the random forest model selected performs better than the other model selected. For the random forest model, the RMSE on the training set is `r round(sqrt(mean(rf_reg_best_wkend$mse)), 4)`, which doesn't suggest the risk of overfitting. However, only 41% of variace could be explained by the selected random forest model. This is probably again due to the fact that we only have very limited features in our original dataset.

### 4.1.3 Impact of Features

Now that we have the regression model constructed, we would like to further understand the impact of different features.

**Weekday**
```{r, fig.width=7,fig.width=7}
varImpPlot(rf_reg_best_wkday)

```

The most important factors are the time when the transaction happened and whether the time is the rush hours - this is not surprising. What's interesting is that the weather related variables follow right after. We use partial dependence plot here to further understand the impact of weather related variables.

**Weekday**
```{r, fig.width=7,fig.width=7}
par(mfrow = c(2, 2))
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'wdir')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'wspd')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'rhum')
partialPlot(rf_reg_best_wkday, df_weekday[train_wkday, ], x.var = 'temp')


```

From the partial dependence plot for Weekday data, we could learn that:

1. Temperature seems to have a U-shape effect on number of transactions. When the temperature is approximately below 2 Celsius, number of transactions decreases as temperature increases; when it is approximately above 2 Celsius, number of transactions increases as temperature increases. Potentially because during weekdays, there could be increasing number of transactions during rush hour, especially in the morning, when the temperature tends to be low. 

2. When there is north wind or north-west wind (270-350), the sales will be higher than the case when the wind is blowing from other directions.

3. Average-level humidity seems to have a positive effect on number of transactions; however, when humidity is close to or over 90, which suggests raining weather, there is a sharp decrease, showing a negative effect on number of transactions. 

4. Stronger wind has negative impact on the sales

To sum up, the most important variable in forecasting the number of transactions at a given time is whether it is during the rush hour. Also, different weather related variables have different impact on the number of transaction - so a practical suggestion to the bakery is to watch the weather forecast and be prepared.

**Weekend**
```{r, fig.width=7,fig.width=7}
varImpPlot(rf_reg_best_wkend)

```

Similar as the case of weekdays, the hour when a transaction happened and whether it is during rush-hours are the most important features. Then the weather related variables also contribute in the model.

**Weekend**
```{r, fig.width=7,fig.width=7}
par(mfrow = c(2, 2))
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'wdir')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'wspd')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'rhum')
partialPlot(rf_reg_best_wkend, df_weekend[train_wkend, ], x.var = 'temp')


```

From the partial dependence plot for Weekend data, we could learn that:

1. Interestingly, in contrast to the data in weekdays, temperature no longer has a U-shape effect; instead, overall, as temperature increases, number of transactions increases. That's probably because no one needs to go buy breakfast in a cold Saturday or Sunday morning.

2. In weekends, when there is north wind or north-west wind (270-350), there is a huge increase in number of transactions. 

3. In weekends, as humidity increases, number of transactions decreases overall. Indeed, it decreases dramatically once the humidity is higher than 40. The reason why it is different comparing with weekday would be that only few people will hang out if weather condition is not good in a weekend.

4. Just as weekdays, wind speed has a negative effect on number of transactions.

Comparing with the weekday case, the humidity and temperature impact the number of transactions differently. People have to go to work in weekdays regardless of the weather. But during weekend, the weather condition will play a more important role when people decide whether to go to the bakery.

## 4.2 Market Basket Analysis

The objective of this analysis is to identify possible association rules. For example, assuming we identify from the data that people who buy cake tend to buy cookie at the same time, then the bakery could consider to place these two item closer in the shop, or create bundle sales in order to boost the sales.

We have already re-grouped different items, so for this analysis, we will start from looking at the association rules among item types.

```{r}
transactions_Item_Type <- as(df_merged_mba_item_type$basket, "transactions")
#inspect(transactions_Item_Type[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS
rules_Item_Type <- apriori(transactions_Item_Type, 
                 parameter = list(support = 0.05, 
                                  confidence = 0.025,
                                  minlen = 2))
# Remove redundant rules
rules_Item_Type <- rules_Item_Type[!is.redundant(rules_Item_Type)]

# Reorder by Lift and display
rules_Item_Type_dt <- data.table( lhs = labels( lhs(rules_Item_Type) ), 
                        rhs = labels( rhs(rules_Item_Type) ), 
                        quality(rules_Item_Type) )[ order(-lift), ]
head(rules_Item_Type_dt, 10)
#rules_Item_Type_dt

```

From the table we observe that the highest lift of all rules is still smaller than 1. That says, no valid association rules identified among the item types.

Then we focus our market basket analysis on the detail list of items, instead of the category. Here we will analyze weekdays and weekends separately, as we are expecting different purchase behaviors.

### 4.2.1 Weekday Dataset Analysis and Discussion

**Weekday**

```{r}
transactions_Item <- as(mba_wkday$basket, "transactions")
#inspect(transactions_Item[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS
rules_Item <- apriori(transactions_Item, 
                 parameter = list(support = 0.005, 
                                  confidence = 0.25,
                                  minlen = 2)
                 )

# Remove redundant rules
rules_Item <- rules_Item[!is.redundant(rules_Item)]

# Reorder by Lift and display
rules_Item_dt_wkday <- data.table( lhs = labels( lhs(rules_Item) ), 
                        rhs = labels( rhs(rules_Item) ), 
                        quality(rules_Item) )[ order(-lift), ]
#rules_Item_dt_wkday%>%filter(lift>1)
head(rules_Item_dt_wkday, 20)



```

```{r}
subrules2 <- head(sort(rules_Item, by = "lift"), 10)
ig <- plot( subrules2, method="graph",
            control=list(type="items") )

```

Few observations could be made from the table:

During weekdays

1. People who buy drinks, such as coffee, juice, tea, and mineral water, tends to buy Sandwich

2. People who buy coffee and juice or juice and tea tends to buy cookies. Normally client won't buy two drinks together if it is only for him or herself. We suspect in this case, it is someone buying drinks and cookies to share with friends.

3. People who buy lunch, such as cake and soup, bread and soup, Spanish brunch, chicken stew, and soup alone, tends to buy tea at the same time. 

4. People who buy sweets, such as scone and cake, tends to buy tea at the same time.

To further explore the finding #4, we plot the distribution of transactions including scone or cake by hours.

```{r}
p1<-mba_wkday%>%filter(Item=='Scone')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkday$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Scone by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p2<-mba_wkday%>%filter(Item=='Cake')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkday$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cake by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)


grid.arrange(p1, p2,
             nrow=1,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours",
                            gp=gpar(fontsize=14,font=3)))

```
It turns out that scone and cake are sold more in the afternoon. The association rules between scone, cake and tea probably refers to the snack in the afternoon.

Based on the above, maybe we could recommend the bakery, for weekdays:

1. To create bundles sales for drinks and sandwiches

2. To create bundles sales for mulitple drinks and cookies or cakes

3. To create bundles sales for lunch and tea

4. To create bundles sales for afternoon snack and tea 

### 4.2.2 Weekend Dataset Analysis and Discussion

**Weekend**
```{r}
transactions_Item <- as(mba_wkend$basket, "transactions")
#inspect(transactions_Item[1])

# Set the threshold low to identify more rules
# Set minlen = 2 to make sure there are items in LHS
rules_Item_wkend <- apriori(transactions_Item, 
                 parameter = list(support = 0.01, 
                                  confidence = 0.25,
                                  minlen = 2)
                 )

# Remove redundant rules
rules_Item_wkend <- rules_Item_wkend[!is.redundant(rules_Item_wkend)]

# Reorder by Lift and display
rules_Item_dt_wkend <- data.table( lhs = labels( lhs(rules_Item_wkend) ), 
                        rhs = labels( rhs(rules_Item_wkend) ), 
                        quality(rules_Item_wkend) )[ order(-lift), ]
#rules_Item_dt_wkend%>%filter(lift>1)
head(rules_Item_dt_wkend, 20)
#rules_Item_dt


```

```{r}
# Graph for 10 rules - Weekend
subrules2 <- head(sort(rules_Item_wkend, by = "lift"), 10)
ig <- plot( subrules2, method="graph",
            control=list(type="items") )

```

In weekends, the association rules concentrate on few items, including cake, tea, coffee, hot chocolate, and scone. One thing worth-noting is that cake is in the center of the graph. In other words, people tends to buy cake when they are purchasing bunch of other items. The association rules also suggest that people sometime buy items to share with others, as there are transactions including both coffee and tea or coffee and hot chocolate.

Since cake is the center of the associations, we are curious when the sales of cakes peak and how the sales of different drinks associated with cakes vary by the hours of the day. 

```{r}
p3<-mba_wkend%>%filter(Item=='Cake')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Cake by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p4<-mba_wkend%>%filter(Item=='Coffee')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Coffee by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p5<-mba_wkend%>%filter(Item=='Tea')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Tea by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p6<-mba_wkend%>%filter(Item=='Hot chocolate')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Hot chocolate by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

grid.arrange(p3,p4, p5,p6,
             nrow=2,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours",
                            gp=gpar(fontsize=14,font=3)))

```
From the figures above, coffee and hot chocolate are sold more in the morning, as opposed to tea and hot chocolate. Sales of cake peak in the afternoon. It is likely that in weekends, people hang out together at the bakery to get the cake and multiple drinks. The bakery could make a special weekend afternoon combo for cake and drinks.

To further explore the top association between Farm House and Medialuna, we plot the distribution of these two items by hours.

```{r}
p1<-mba_wkend%>%filter(Item=='Medialuna')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Medialuna by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

p2<-mba_wkend%>%filter(Item=='Farm House')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Farm House by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

grid.arrange(p1, p2,
             nrow=1,ncol=2,
             top = textGrob("Average Transaction Frequency by Hours",
                            gp=gpar(fontsize=14,font=3)))
```

It is obvious from the figure above that these two items are purchased in the mornings as breakfast. 
- To encourage more sales of these two items in the afternoons, maybe include them as an option in the combo with the cake and drinks.

```{r}
mba_wkend%>%filter(Item=='Pastry')%>%
  group_by(as.factor(Hour))%>%
  mutate(Avg_No_of_transaction = length(unique(Transaction))/length(unique(mba_wkend$Transaction))) %>%
  ggplot(aes(x = as.factor(Hour), y = Avg_No_of_transaction, fill = as.factor(Hour))) +
  geom_bar(stat = 'identity',alpha=0.8) +
  labs(title = 'Average Number of Transactions of Pastry by Hour',
       x = 'Hours', y = 'Avg No. of Transaction') +
  scale_fill_manual(values = colorRampPalette(c('#ecc78d','#2e0a05'))(nb.cols)) +
  theme_minimal() +
  theme(legend.position = "none",
plot.title = element_text(size=8),
axis.title.x = element_text(size=8),
axis.title.y = element_text(size=8)
)

```

Finally, we also explore the distribution of the sales of Pastry by hours since Pastry is also often associated with coffee and hot chocolate. Just as Medialuna, pastry is also often bought in the mornings. To boost the sales of pastry in the afternoons, we also recommend including pastry with the afternoon combo set with cake and drinks. Or since hot chocolate is often bought in the afternoons, we could also recommend creating combos of hot chocolate and pastry to encourage its sales in the afternoons.  

Based on the observations, the recommendations to the bakery would be:

1. During weekends, place the cake in a visible place closer to the cashier

2. Keep the combo of multiple drinks and cake 


## 4.3 Clustering Analysis and Discussion

In this section, we will explore the different clusters of transactions, if there is any.

### 4.3.1 Clustering on Weekday Dataset

**Weekday**
```{r}

fviz_nbclust(
  cluster_wkday,
  kmeans,
  k.max = 10,
  method = "wss"
)


fviz_nbclust(
  cluster_wkday,
  kmeans,
  k.max = 10,
  method = "silhouette"
)

```

Both figures suggest that the best K should be 2. So we will start with k = 2.

```{r}

km_out_2 <- kmeans(cluster_wkday, 2, nstart = 25)

fviz_cluster(km_out_2, cluster_wkday, geom = 'point', ellipse.type = 'norm',
             main = 'Weekday Dataset: K-Means Clustering Results with K=2')

```

Apparently, PCA might not be the ideal way to extract the features for visualization. The first and second principle components together could only explain 37% of variance.

Then to understand how well we split the transactions into two clusters, here we compare the centers of the clusters.

```{r}
km_out_2$centers

```

The major difference between the center of two clusters is the direction of wind. This is difficult to explain and therefore we doubt whether the current clustering makes sense.

### 4.3.2 Clustering on Weekend Dataset

**Weekend**

```{r}
fviz_nbclust(
  cluster_wkend,
  kmeans,
  k.max = 10,
  method = "wss"
)


fviz_nbclust(
  cluster_wkend,
  kmeans,
  k.max = 10,
  method = "silhouette"
)

```

Both figures suggest that the best K should be 2. So we will start with k = 2.

```{r}

km_out_2 <- kmeans(cluster_wkend, 2, nstart = 25)

fviz_cluster(km_out_2, cluster_wkend, geom = 'point', ellipse.type = 'norm',
             main = 'Weekday Dataset: K-Means Clustering Results with K=2')

```

Same as the weekdays dataset, PCA might not be the ideal way to extract the features for visualization. The first and second principle components together could only explain 38% of variance.

Then to understand how well we split the transactions into two clusters, here we compare the centers of the clusters.

```{r}
km_out_2$centers

```

Again, the major difference between the center of two clusters is the direction of wind. This is difficult to explain and therefore we doubt whether the current clustering makes sense.

To sum up, we are not able to come up with a meaningful split of transactions into clusters. On one hand, we only tried with k-means. On the other hand, we have very limited features in the original dataset.

# 5 Conclusion & Next Steps

In a nutshell, with the bakery transaction data and weather record data, we first explored how different features influencing the number of transactions, especially those weather-related variables. Then we dived into the association rules from the transaction history, identified purchasing patterns, and provided recommendations for the bakery to further increase sales. Lastly we tried to cluster the transactions into different segments, but the result is not satisfying.

The major next steps of this analysis are:

1. When predicting the number of transactions, the selected random forest model could only explain 30% - 40% of variance. We will need to either acquire more data beyond Oct 2016 and Apr 2017, or get more new features. Our current features only include date, time, name of items, and weather. If we could manage to get the information of the client, we could further improve our model to perform the prediction task.

2. The association rules we identified has low support in general. If we could get more data, we might be able to further explore the the possible rules.

3. We will need features to reshape the clustering analysis. Also as next step, we could try other clustering methods, such as GMM or DPGMM.

4. Whether a day is a public holiday or not is not significant in any of our analysis. Part of the reason is that our dataset only lasts 6 months. If we could acquire more data, covering several years, we could further explore the impact of holiday on the sales of the bakery.


